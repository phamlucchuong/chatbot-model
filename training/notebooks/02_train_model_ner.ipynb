{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dc85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyvi\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3334af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_model(train_df, n_iter=30):\n",
    "    \"\"\"\n",
    "    Hu·∫•n luy·ªán NER model t·ª´ m·ªôt DataFrame (ƒë·ªçc t·ª´ file .jsonl).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Chuy·ªÉn ƒë·ªïi DataFrame sang ƒë·ªãnh d·∫°ng list (text, annot) ---\n",
    "    # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ h√†m t∆∞∆°ng th√≠ch v·ªõi d·ªØ li·ªáu t·ª´ DataFrame\n",
    "    TRAIN_DATA = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        TRAIN_DATA.append(\n",
    "            (row['text'], {\"entities\": row['entities']})\n",
    "        )\n",
    "    \n",
    "    # --- 2. T·∫°o model v√† pipeline ---\n",
    "    # D√≤ng n√†y y√™u c·∫ßu th∆∞ vi·ªán 'pyvi' ph·∫£i ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
    "    nlp = spacy.blank(\"vi\")\n",
    "    print(\"ƒê√£ t·∫°o m√¥ h√¨nh 'vi' tr·ªëng.\")\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # --- 3. T·ª∞ ƒê·ªòNG TH√äM T·∫§T C·∫¢ LABEL (Thay ƒë·ªïi quan tr·ªçng) ---\n",
    "    # T·ª± ƒë·ªông t√¨m t·∫•t c·∫£ c√°c nh√£n duy nh·∫•t t·ª´ d·ªØ li·ªáu\n",
    "    all_labels = set()\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            all_labels.add(ent[2]) # ent[2] l√† nh√£n (vd: 'SYMPTOM', 'DISEASE')\n",
    "            \n",
    "    for label in all_labels:\n",
    "        ner.add_label(label)\n",
    "        \n",
    "    print(f\"ƒê√£ th√™m c√°c nh√£n v√†o pipeline: {all_labels}\")\n",
    "\n",
    "    # --- 4. B·∫Øt ƒë·∫ßu training (Gi·ªØ nguy√™n logic c·ªßa b·∫°n) ---\n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán NER model...\")\n",
    "    \n",
    "    optimizer = nlp.begin_training()\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        for iteration in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # S·ª≠ d·ª•ng TRAIN_DATA (list ƒë√£ chuy·ªÉn ƒë·ªïi) ·ªü ƒë√¢y\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            \n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    try:\n",
    "                        doc = nlp.make_doc(text)\n",
    "                        example = Example.from_dict(doc, annotations)\n",
    "                        examples.append(example)\n",
    "                    except Exception as e:\n",
    "                        # B·ªè qua n·∫øu c√≥ l·ªói ch√∫ th√≠ch (v√≠ d·ª•: entity n·∫±m ngo√†i c√¢u)\n",
    "                        print(f\"L·ªói khi t·∫°o Example: {e} - D·ªØ li·ªáu: {text[:50]}...\")\n",
    "                \n",
    "                # C·∫≠p nh·∫≠t model\n",
    "                if examples: # Ch·ªâ update n·∫øu c√≥ examples h·ª£p l·ªá\n",
    "                    nlp.update(examples, drop=0.35, sgd=optimizer, losses=losses)\n",
    "            \n",
    "            # Ch·ªâ in loss n·∫øu c√≥ 'ner' trong losses\n",
    "            if (iteration + 1) % 5 == 0 and 'ner' in losses:\n",
    "                print(f\"Iteration {iteration + 1}/{n_iter} - Loss: {losses['ner']:.4f}\")\n",
    "    \n",
    "    print(\"‚úì Ho√†n th√†nh training!\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbea967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text  \\\n",
      "0                         T√¥i b·ªã ƒëau ƒë·∫ßu v√† s·ªët nh·∫π   \n",
      "1               M·∫•y h√¥m nay t√¥i ho khan v√† ƒëau h·ªçng   \n",
      "2           T√¥i b·ªã s·ªï m≈©i v√† ngh·∫πt m≈©i m·∫•y ng√†y nay   \n",
      "3                  C∆° th·ªÉ t√¥i r·∫•t m·ªát m·ªèi v√† u·ªÉ o·∫£i   \n",
      "4  T√¥i c√≥ tri·ªáu ch·ª©ng ·ªõn l·∫°nh v√† ƒëau nh·ª©c to√†n th√¢n   \n",
      "\n",
      "                                 entities  \n",
      "0   [[7, 14, SYMPTOM], [18, 25, SYMPTOM]]  \n",
      "1  [[16, 23, SYMPTOM], [27, 35, SYMPTOM]]  \n",
      "2   [[7, 13, SYMPTOM], [17, 26, SYMPTOM]]  \n",
      "3  [[15, 22, SYMPTOM], [26, 32, SYMPTOM]]  \n",
      "4  [[19, 26, SYMPTOM], [30, 48, SYMPTOM]]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"../data/processed/train_data.spacy.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73f674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ t·∫°o m√¥ h√¨nh 'vi' tr·ªëng.\n",
      "ƒê√£ th√™m c√°c nh√£n v√†o pipeline: {'DISEASE', 'SYMPTOM'}\n",
      "üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán NER model...\n",
      "Iteration 5/30 - Loss: 57.3066\n",
      "Iteration 10/30 - Loss: 40.6455\n",
      "Iteration 15/30 - Loss: 14.1750\n",
      "Iteration 20/30 - Loss: 18.7759\n",
      "Iteration 25/30 - Loss: 8.9134\n",
      "Iteration 30/30 - Loss: 4.1295\n",
      "‚úì Ho√†n th√†nh training!\n",
      "‚úì ƒê√£ l∆∞u model v√†o th∆∞ m·ª•c 'ner_model'\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = train_ner_model(df, n_iter=30)\n",
    "\n",
    "# L∆∞u model\n",
    "model.to_disk(\"../models/spacy_ner_model\")\n",
    "print(\"‚úì ƒê√£ l∆∞u model v√†o th∆∞ m·ª•c 'ner_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4565b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Test model v·ªõi c√¢u m·∫´u:\n",
      "\n",
      "üìù Input: T√¥i b·ªã s·ªët v√† ho nhi·ªÅu\n",
      "üîç Tri·ªáu ch·ª©ng t√¨m ƒë∆∞·ª£c:\n",
      "   - s·ªët [SYMPTOM]\n",
      "   - ho nhi·ªÅu [SYMPTOM]\n",
      "\n",
      "üìù Input: Em b·ªã ƒëau b·ª•ng d·ªØ d·ªôi\n",
      "üîç Tri·ªáu ch·ª©ng t√¨m ƒë∆∞·ª£c:\n",
      "   - ƒëau b·ª•ng d·ªØ d·ªôi [SYMPTOM]\n",
      "\n",
      "üìù Input: B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng ch√≥ng m·∫∑t v√† bu·ªìn n√¥n\n",
      "üîç Tri·ªáu ch·ª©ng t√¨m ƒë∆∞·ª£c:\n",
      "   - bu·ªìn n√¥n [SYMPTOM]\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "print(\"\\nüìã Test model v·ªõi c√¢u m·∫´u:\")\n",
    "test_texts = [\n",
    "    \"T√¥i b·ªã s·ªët v√† ho nhi·ªÅu\",\n",
    "    \"Em b·ªã ƒëau b·ª•ng d·ªØ d·ªôi\",\n",
    "    \"B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng ch√≥ng m·∫∑t v√† bu·ªìn n√¥n\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    doc = model(text)\n",
    "    print(f\"\\nüìù Input: {text}\")\n",
    "    print(f\"üîç Tri·ªáu ch·ª©ng t√¨m ƒë∆∞·ª£c:\")\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f\"   - {ent.text} [{ent.label_}]\")\n",
    "    else:\n",
    "        print(\"   (Kh√¥ng t√¨m th·∫•y)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
