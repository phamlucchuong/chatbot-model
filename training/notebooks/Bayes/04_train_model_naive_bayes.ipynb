{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes Classifier - Optimized\n",
    "\n",
    "**Author:** phamlucchuong  \n",
    "**Date:** 2025-01-18  \n",
    "**Dataset:** raw_data_bayes.csv (with disease_id)  \n",
    "\n",
    "Training Naive Bayes v·ªõi:\n",
    "- Multiple model comparison\n",
    "- Cross-validation\n",
    "- Parallel processing\n",
    "- Detailed evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ NAIVE BAYES CLASSIFIER TRAINING\n",
      "======================================================================\n",
      "Start time: 2025-11-19 11:50:42\n",
      "Author: phamlucchuong\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ NAIVE BAYES CLASSIFIER TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Author: phamlucchuong\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÇ LOADING DATA\n",
      "======================================================================\n",
      "‚úì Loaded binary data: (2173, 627)\n",
      "‚úì Loaded symptom mapping: 625 symptoms\n",
      "‚úì Loaded disease mapping: 20 diseases\n",
      "\n",
      "üìä Dataset Info:\n",
      "   Samples: 2173\n",
      "   Features: 625\n",
      "   Classes: 21\n",
      "   Columns: ['disease_id', 'disease', 'Ban_ƒë·ªè', 'B·ªã_b√≠t_m≈©i', 'B·ªã_nh·ª©c_ƒë·∫ßu'] ... (+622 more)\n",
      "\n",
      "üìã Sample data (first 3 rows, first 8 columns):\n",
      "  disease_id                disease  Ban_ƒë·ªè  B·ªã_b√≠t_m≈©i  B·ªã_nh·ª©c_ƒë·∫ßu  B·ªã_s·ªët  \\\n",
      "0       D001  C·∫£m l·∫°nh th√¥ng th∆∞·ªùng       0           0            0       0   \n",
      "1       D001  C·∫£m l·∫°nh th√¥ng th∆∞·ªùng       0           0            0       0   \n",
      "2       D001  C·∫£m l·∫°nh th√¥ng th∆∞·ªùng       0           0            0       0   \n",
      "\n",
      "   B·ªã_s·ªï_m≈©i  Ch·∫£y_m≈©i  \n",
      "0          0         0  \n",
      "1          0         0  \n",
      "2          0         0  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÇ LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load binary data\n",
    "df = pd.read_csv(\"../../data/processed/naive_bayes_data.csv\")\n",
    "print(f\"‚úì Loaded binary data: {df.shape}\")\n",
    "\n",
    "# Load symptom mapping\n",
    "with open(\"../../data/processed/symptom_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    symptom_mapping = json.load(f)\n",
    "\n",
    "all_symptoms = symptom_mapping[\"all_symptoms\"]\n",
    "print(f\"‚úì Loaded symptom mapping: {len(all_symptoms)} symptoms\")\n",
    "\n",
    "# Load disease mapping\n",
    "with open(\"../../data/processed/disease_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    disease_mapping = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded disease mapping: {len(disease_mapping)} diseases\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Info:\")\n",
    "print(f\"   Samples: {len(df)}\")\n",
    "print(f\"   Features: {len(all_symptoms)}\")\n",
    "print(f\"   Classes: {df['disease'].nunique()}\")\n",
    "print(f\"   Columns: {df.columns.tolist()[:5]} ... (+{len(df.columns)-5} more)\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüìã Sample data (first 3 rows, first 8 columns):\")\n",
    "print(df.iloc[:3, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ph√¢n t√≠ch ph√¢n b·ªë d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä DATA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìà Ph√¢n b·ªë m·∫´u theo b·ªánh:\n",
      "\n",
      "Disease                                       ID        Samples       %\n",
      "======================================================================\n",
      "B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim) D014          158    7.3%\n",
      "B·ªánh TƒÉng huy·∫øt √°p (Tim m·∫°ch)                 D013          135    6.2%\n",
      "Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            D011          130    6.0%\n",
      "Th·ªßy ƒë·∫≠u (Chickenpox)                         D012          118    5.4%\n",
      "B·ªánh Ph·ªïi t·∫Øc ngh·∫Ωn m·∫°n t√≠nh (COPD)           D017          117    5.4%\n",
      "Vi√™m kh·ªõp m·∫°n t√≠nh                            D020          115    5.3%\n",
      "ƒê√°i th√°o ƒë∆∞·ªùng (Ti·ªÉu ƒë∆∞·ªùng)                   D015          110    5.1%\n",
      "Vi√™m lo√©t d·∫° d√†y v√† T√° tr√†ng                  D018          101    4.6%\n",
      "B·ªánh Lao ph·ªïi                                 D005          100    4.6%\n",
      "C·∫£m l·∫°nh th√¥ng th∆∞·ªùng                         D001          100    4.6%\n",
      "Vi√™m ph·∫ø qu·∫£n c·∫•p                             D004          100    4.6%\n",
      "Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p                  D003          100    4.6%\n",
      "C·∫£m c√∫m                                       D002          100    4.6%\n",
      "B·ªánh Ti√™u ch·∫£y c·∫•p                            D010          100    4.6%\n",
      "S·ªët r√©t                                       D007          100    4.6%\n",
      "B·ªánh Tay - Ch√¢n - Mi·ªáng                       D008          100    4.6%\n",
      "Vi√™m K·∫øt M·∫°c                                  D009          100    4.6%\n",
      "S·ªët xu·∫•t huy·∫øt                                D006          100    4.6%\n",
      "Ung th∆∞ (Gan, Ph·ªïi, D·∫° d√†y)                   D016           96    4.4%\n",
      "B·ªánh G√∫t (Gout)                               D019           92    4.2%\n",
      "B·ªánh Ph·ªïi t·∫Øc_ngh·∫Ωn_m·∫°n_t√≠nh (COPD)           N/A             1    0.0%\n",
      "\n",
      "üìä Th·ªëng k√™:\n",
      "   Mean:   103.5 samples/disease\n",
      "   Median: 100.0 samples/disease\n",
      "   Min:    1 samples\n",
      "   Max:    158 samples\n",
      "   Std:    28.3\n",
      "\n",
      "üîç Ph√¢n t√≠ch tri·ªáu ch·ª©ng:\n",
      "   Top 10 tri·ªáu ch·ª©ng ph·ªï bi·∫øn nh·∫•t:\n",
      "    1. m·ªát_m·ªèi                                  202 l·∫ßn (9.3%)\n",
      "    2. s·ªët_nh·∫π                                  164 l·∫ßn (7.5%)\n",
      "    3. ch√≥ng_m·∫∑t_nh·∫π                            113 l·∫ßn (5.2%)\n",
      "    4. s·ªët_v·ª´a                                   94 l·∫ßn (4.3%)\n",
      "    5. s·ªët_cao                                   92 l·∫ßn (4.2%)\n",
      "    6. ch√°n_ƒÉn                                   87 l·∫ßn (4.0%)\n",
      "    7. h·ªìi_h·ªôp                                   84 l·∫ßn (3.9%)\n",
      "    8. kh√≥_th·ªü_khi_g·∫Øng_s·ª©c                      76 l·∫ßn (3.5%)\n",
      "    9. u·ªÉ_o·∫£i                                    74 l·∫ßn (3.4%)\n",
      "   10. ·ªõn_l·∫°nh                                   72 l·∫ßn (3.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ph√¢n b·ªë b·ªánh\n",
    "disease_counts = df['disease'].value_counts()\n",
    "\n",
    "print(f\"\\nüìà Ph√¢n b·ªë m·∫´u theo b·ªánh:\")\n",
    "print(f\"\\n{'Disease':<45} {'ID':<8} {'Samples':>8} {'%':>7}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# T·∫°o reverse mapping (disease_name -> disease_id)\n",
    "reverse_disease_map = {v: k for k, v in disease_mapping.items()}\n",
    "\n",
    "for disease, count in disease_counts.items():\n",
    "    disease_id = reverse_disease_map.get(disease, \"N/A\")\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"{disease:<45} {disease_id:<8} {count:>8} {percentage:>6.1f}%\")\n",
    "\n",
    "print(\"\\nüìä Th·ªëng k√™:\")\n",
    "print(f\"   Mean:   {disease_counts.mean():.1f} samples/disease\")\n",
    "print(f\"   Median: {disease_counts.median():.1f} samples/disease\")\n",
    "print(f\"   Min:    {disease_counts.min()} samples\")\n",
    "print(f\"   Max:    {disease_counts.max()} samples\")\n",
    "print(f\"   Std:    {disease_counts.std():.1f}\")\n",
    "\n",
    "# Ph√¢n t√≠ch tri·ªáu ch·ª©ng\n",
    "print(f\"\\nüîç Ph√¢n t√≠ch tri·ªáu ch·ª©ng:\")\n",
    "symptom_counts = df[all_symptoms].sum().sort_values(ascending=False)\n",
    "print(f\"   Top 10 tri·ªáu ch·ª©ng ph·ªï bi·∫øn nh·∫•t:\")\n",
    "for i, (symptom, count) in enumerate(symptom_counts.head(10).items(), 1):\n",
    "    print(f\"   {i:2d}. {symptom:<40} {count:>3} l·∫ßn ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chu·∫©n b·ªã d·ªØ li·ªáu cho training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "‚úì Features (X): (2173, 625)\n",
      "‚úì Labels (y): (2173,)\n",
      "\n",
      "‚ö†Ô∏è  Removing 1 classes with < 2 samples:\n",
      "   - B·ªánh Ph·ªïi t·∫Øc_ngh·∫Ωn_m·∫°n_t√≠nh (COPD) (N/A): 1 samples\n",
      "\n",
      "‚úì After filtering: 2172 samples, 20 classes\n",
      "\n",
      "‚úì Applied Laplace smoothing (alpha=0.01)\n",
      "\n",
      "üìä Final data shape:\n",
      "   X: (2172, 625)\n",
      "   y: (2172,)\n",
      "   Classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# T√°ch features v√† labels\n",
    "X = df[all_symptoms].values\n",
    "y = df[\"disease\"].values\n",
    "\n",
    "print(f\"\\n‚úì Features (X): {X.shape}\")\n",
    "print(f\"‚úì Labels (y): {y.shape}\")\n",
    "\n",
    "# L·ªçc b·ªè classes c√≥ √≠t h∆°n 2 m·∫´u\n",
    "min_samples = 2\n",
    "disease_counts = Counter(y)\n",
    "valid_classes = [d for d, count in disease_counts.items() if count >= min_samples]\n",
    "removed_classes = [d for d, count in disease_counts.items() if count < min_samples]\n",
    "\n",
    "if removed_classes:\n",
    "    print(f\"\\n‚ö†Ô∏è  Removing {len(removed_classes)} classes with < {min_samples} samples:\")\n",
    "    for disease in removed_classes:\n",
    "        disease_id = reverse_disease_map.get(disease, \"N/A\")\n",
    "        print(f\"   - {disease} ({disease_id}): {disease_counts[disease]} samples\")\n",
    "    \n",
    "    mask = np.isin(y, valid_classes)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(f\"\\n‚úì After filtering: {len(X)} samples, {len(valid_classes)} classes\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All classes have >= {min_samples} samples\")\n",
    "\n",
    "# Laplace smoothing\n",
    "smoothing_value = 0.01\n",
    "X = X + smoothing_value\n",
    "print(f\"\\n‚úì Applied Laplace smoothing (alpha={smoothing_value})\")\n",
    "\n",
    "print(f\"\\nüìä Final data shape:\")\n",
    "print(f\"   X: {X.shape}\")\n",
    "print(f\"   y: {y.shape}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÇÔ∏è  TRAIN/TEST SPLIT\n",
      "======================================================================\n",
      "‚úì Using stratified split\n",
      "\n",
      "üìä Split summary:\n",
      "   Method: Stratified\n",
      "   Test size: 20%\n",
      "   Random state: 42\n",
      "\n",
      "   Train: 1737 samples (80.0%)\n",
      "   Test:   435 samples (20.0%)\n",
      "\n",
      "üìà Class distribution:\n",
      "   Train: 20 unique classes\n",
      "   Test:  20 unique classes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÇÔ∏è  TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# Try stratified split\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    split_method = \"Stratified\"\n",
    "    print(f\"‚úì Using stratified split\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ö†Ô∏è  Stratified split failed: {e}\")\n",
    "    print(f\"   ‚Üí Using random split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    split_method = \"Random\"\n",
    "\n",
    "print(f\"\\nüìä Split summary:\")\n",
    "print(f\"   Method: {split_method}\")\n",
    "print(f\"   Test size: {test_size*100:.0f}%\")\n",
    "print(f\"   Random state: {random_state}\")\n",
    "print(f\"\\n   Train: {len(X_train):>4} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test:  {len(X_test):>4} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Class distribution\n",
    "train_classes = len(np.unique(y_train))\n",
    "test_classes = len(np.unique(y_test))\n",
    "\n",
    "print(f\"\\nüìà Class distribution:\")\n",
    "print(f\"   Train: {train_classes} unique classes\")\n",
    "print(f\"   Test:  {test_classes} unique classes\")\n",
    "\n",
    "if train_classes != test_classes:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Train and test have different number of classes!\")\n",
    "    missing_in_test = set(y_train) - set(y_test)\n",
    "    if missing_in_test:\n",
    "        print(f\"   Classes missing in test set: {missing_in_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: MultinomialNB (Œ±=0.1)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9942\n",
      "‚úì Test Accuracy:   0.9862\n",
      "‚úì CV Accuracy:     0.9856 (¬±0.0059)\n",
      "‚úì Precision:       0.9870\n",
      "‚úì Recall:          0.9862\n",
      "‚úì F1-Score:        0.9863\n",
      "‚è±Ô∏è  Training time:   0.0185s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: MultinomialNB (Œ±=0.5)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9919\n",
      "‚úì Test Accuracy:   0.9839\n",
      "‚úì CV Accuracy:     0.9799 (¬±0.0043)\n",
      "‚úì Precision:       0.9848\n",
      "‚úì Recall:          0.9839\n",
      "‚úì F1-Score:        0.9839\n",
      "‚è±Ô∏è  Training time:   0.0098s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: MultinomialNB (Œ±=1.0)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9908\n",
      "‚úì Test Accuracy:   0.9816\n",
      "‚úì CV Accuracy:     0.9729 (¬±0.0045)\n",
      "‚úì Precision:       0.9828\n",
      "‚úì Recall:          0.9816\n",
      "‚úì F1-Score:        0.9815\n",
      "‚è±Ô∏è  Training time:   0.0093s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: MultinomialNB (Œ±=2.0)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9856\n",
      "‚úì Test Accuracy:   0.9770\n",
      "‚úì CV Accuracy:     0.9603 (¬±0.0051)\n",
      "‚úì Precision:       0.9791\n",
      "‚úì Recall:          0.9770\n",
      "‚úì F1-Score:        0.9768\n",
      "‚è±Ô∏è  Training time:   0.0070s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: ComplementNB (Œ±=0.5)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9620\n",
      "‚úì Test Accuracy:   0.9563\n",
      "‚úì CV Accuracy:     0.9326 (¬±0.0051)\n",
      "‚úì Precision:       0.9607\n",
      "‚úì Recall:          0.9563\n",
      "‚úì F1-Score:        0.9555\n",
      "‚è±Ô∏è  Training time:   0.0077s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training: ComplementNB (Œ±=1.0)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚úì Train Accuracy:  0.9620\n",
      "‚úì Test Accuracy:   0.9563\n",
      "‚úì CV Accuracy:     0.9315 (¬±0.0035)\n",
      "‚úì Precision:       0.9607\n",
      "‚úì Recall:          0.9563\n",
      "‚úì F1-Score:        0.9555\n",
      "‚è±Ô∏è  Training time:   0.0078s\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All models trained in 7.56s\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"MultinomialNB (Œ±=0.1)\": MultinomialNB(alpha=0.1),\n",
    "    \"MultinomialNB (Œ±=0.5)\": MultinomialNB(alpha=0.5),\n",
    "    \"MultinomialNB (Œ±=1.0)\": MultinomialNB(alpha=1.0),\n",
    "    \"MultinomialNB (Œ±=2.0)\": MultinomialNB(alpha=2.0),\n",
    "    \"ComplementNB (Œ±=0.5)\": ComplementNB(alpha=0.5),\n",
    "    \"ComplementNB (Œ±=1.0)\": ComplementNB(alpha=1.0),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    model_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - model_start\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation (n·∫øu c√≥ ƒë·ªß samples)\n",
    "    try:\n",
    "        n_splits = min(3, len(np.unique(y_train)))\n",
    "        if n_splits >= 2:\n",
    "            cv_scores = cross_val_score(\n",
    "                model, X_train, y_train,\n",
    "                cv=n_splits,\n",
    "                scoring='accuracy',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            cv_mean = 0\n",
    "            cv_std = 0\n",
    "    except:\n",
    "        cv_mean = 0\n",
    "        cv_std = 0\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred_test, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'train_time': train_time,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì Train Accuracy:  {train_acc:.4f}\")\n",
    "    print(f\"‚úì Test Accuracy:   {test_acc:.4f}\")\n",
    "    if cv_mean > 0:\n",
    "        print(f\"‚úì CV Accuracy:     {cv_mean:.4f} (¬±{cv_std:.4f})\")\n",
    "    print(f\"‚úì Precision:       {precision:.4f}\")\n",
    "    print(f\"‚úì Recall:          {recall:.4f}\")\n",
    "    print(f\"‚úì F1-Score:        {f1:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Training time:   {train_time:.4f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ All models trained in {total_time:.2f}s\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "                Model Train Acc Test Acc CV Acc Precision Recall     F1 Time (s)\n",
      "MultinomialNB (Œ±=0.1)    0.9942   0.9862 0.9856    0.9870 0.9862 0.9863   0.0185\n",
      "MultinomialNB (Œ±=0.5)    0.9919   0.9839 0.9799    0.9848 0.9839 0.9839   0.0098\n",
      "MultinomialNB (Œ±=1.0)    0.9908   0.9816 0.9729    0.9828 0.9816 0.9815   0.0093\n",
      "MultinomialNB (Œ±=2.0)    0.9856   0.9770 0.9603    0.9791 0.9770 0.9768   0.0070\n",
      " ComplementNB (Œ±=0.5)    0.9620   0.9563 0.9326    0.9607 0.9563 0.9555   0.0077\n",
      " ComplementNB (Œ±=1.0)    0.9620   0.9563 0.9315    0.9607 0.9563 0.9555   0.0078\n",
      "\n",
      "======================================================================\n",
      "üèÜ BEST MODEL: MultinomialNB (Œ±=0.1)\n",
      "======================================================================\n",
      "   Test Accuracy:  0.9862\n",
      "   Precision:      0.9870\n",
      "   Recall:         0.9862\n",
      "   F1-Score:       0.9863\n",
      "   Training time:  0.0185s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for name, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': f\"{res['train_acc']:.4f}\",\n",
    "        'Test Acc': f\"{res['test_acc']:.4f}\",\n",
    "        'CV Acc': f\"{res['cv_mean']:.4f}\" if res['cv_mean'] > 0 else \"N/A\",\n",
    "        'Precision': f\"{res['precision']:.4f}\",\n",
    "        'Recall': f\"{res['recall']:.4f}\",\n",
    "        'F1': f\"{res['f1']:.4f}\",\n",
    "        'Time (s)': f\"{res['train_time']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['test_acc'])\n",
    "best_result = results[best_model_name]\n",
    "best_model = best_result['model']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   Test Accuracy:  {best_result['test_acc']:.4f}\")\n",
    "print(f\"   Precision:      {best_result['precision']:.4f}\")\n",
    "print(f\"   Recall:         {best_result['recall']:.4f}\")\n",
    "print(f\"   F1-Score:       {best_result['f1']:.4f}\")\n",
    "print(f\"   Training time:  {best_result['train_time']:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä DETAILED EVALUATION - MultinomialNB (Œ±=0.1)\n",
      "======================================================================\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                    B·ªánh G√∫t (Gout)       1.00      1.00      1.00        18\n",
      "                                      B·ªánh Lao ph·ªïi       0.95      0.95      0.95        20\n",
      "                B·ªánh Ph·ªïi t·∫Øc ngh·∫Ωn m·∫°n t√≠nh (COPD)       1.00      1.00      1.00        24\n",
      "                            B·ªánh Tay - Ch√¢n - Mi·ªáng       1.00      1.00      1.00        20\n",
      "B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim)       1.00      0.94      0.97        32\n",
      "                                 B·ªánh Ti√™u ch·∫£y c·∫•p       1.00      1.00      1.00        20\n",
      "                      B·ªánh TƒÉng huy·∫øt √°p (Tim m·∫°ch)       0.90      1.00      0.95        27\n",
      "                                            C·∫£m c√∫m       1.00      1.00      1.00        20\n",
      "                              C·∫£m l·∫°nh th√¥ng th∆∞·ªùng       1.00      1.00      1.00        20\n",
      "                                            S·ªët r√©t       1.00      1.00      1.00        20\n",
      "                                     S·ªët xu·∫•t huy·∫øt       1.00      1.00      1.00        20\n",
      "                              Th·ªßy ƒë·∫≠u (Chickenpox)       1.00      1.00      1.00        24\n",
      "                        Ung th∆∞ (Gan, Ph·ªïi, D·∫° d√†y)       0.95      0.95      0.95        19\n",
      "                                       Vi√™m K·∫øt M·∫°c       1.00      1.00      1.00        20\n",
      "                 Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)       1.00      0.92      0.96        26\n",
      "                       Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p       1.00      1.00      1.00        20\n",
      "                                 Vi√™m kh·ªõp m·∫°n t√≠nh       1.00      1.00      1.00        23\n",
      "                       Vi√™m lo√©t d·∫° d√†y v√† T√° tr√†ng       1.00      1.00      1.00        20\n",
      "                                  Vi√™m ph·∫ø qu·∫£n c·∫•p       0.95      1.00      0.98        20\n",
      "                        ƒê√°i th√°o ƒë∆∞·ªùng (Ti·ªÉu ƒë∆∞·ªùng)       1.00      1.00      1.00        22\n",
      "\n",
      "                                           accuracy                           0.99       435\n",
      "                                          macro avg       0.99      0.99      0.99       435\n",
      "                                       weighted avg       0.99      0.99      0.99       435\n",
      "\n",
      "\n",
      "üìà Per-class Performance:\n",
      "\n",
      "Disease                                       ID          Acc  Samples\n",
      "======================================================================\n",
      "B·ªánh G√∫t (Gout)                               D019     100.0%       18\n",
      "B·ªánh Lao ph·ªïi                                 D005     95.0%       20\n",
      "B·ªánh Ph·ªïi t·∫Øc ngh·∫Ωn m·∫°n t√≠nh (COPD)           D017     100.0%       24\n",
      "B·ªánh Tay - Ch√¢n - Mi·ªáng                       D008     100.0%       20\n",
      "B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim) D014     93.8%       32\n",
      "B·ªánh Ti√™u ch·∫£y c·∫•p                            D010     100.0%       20\n",
      "B·ªánh TƒÉng huy·∫øt √°p (Tim m·∫°ch)                 D013     100.0%       27\n",
      "C·∫£m c√∫m                                       D002     100.0%       20\n",
      "C·∫£m l·∫°nh th√¥ng th∆∞·ªùng                         D001     100.0%       20\n",
      "S·ªët r√©t                                       D007     100.0%       20\n",
      "S·ªët xu·∫•t huy·∫øt                                D006     100.0%       20\n",
      "Th·ªßy ƒë·∫≠u (Chickenpox)                         D012     100.0%       24\n",
      "Ung th∆∞ (Gan, Ph·ªïi, D·∫° d√†y)                   D016     94.7%       19\n",
      "Vi√™m K·∫øt M·∫°c                                  D009     100.0%       20\n",
      "Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            D011     92.3%       26\n",
      "Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p                  D003     100.0%       20\n",
      "Vi√™m kh·ªõp m·∫°n t√≠nh                            D020     100.0%       23\n",
      "Vi√™m lo√©t d·∫° d√†y v√† T√° tr√†ng                  D018     100.0%       20\n",
      "Vi√™m ph·∫ø qu·∫£n c·∫•p                             D004     100.0%       20\n",
      "ƒê√°i th√°o ƒë∆∞·ªùng (Ti·ªÉu ƒë∆∞·ªùng)                   D015     100.0%       22\n",
      "\n",
      "üìä Confusion Matrix (Top 10 most common classes in test set):\n",
      "\n",
      "                                                     B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim)  B·ªánh TƒÉng huy·∫øt √°p (Tim m·∫°ch)  Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)  Th·ªßy ƒë·∫≠u (Chickenpox)  B·ªánh Ph·ªïi t·∫Øc ngh·∫Ωn m·∫°n t√≠nh (COPD)  Vi√™m kh·ªõp m·∫°n t√≠nh  ƒê√°i th√°o ƒë∆∞·ªùng (Ti·ªÉu ƒë∆∞·ªùng)  S·ªët xu·∫•t huy·∫øt  B·ªánh Ti√™u ch·∫£y c·∫•p  B·ªánh Lao ph·ªïi\n",
      "B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim)                                                   30                              2                                   0                      0                                    0                   0                            0               0                   0              0\n",
      "B·ªánh TƒÉng huy·∫øt √°p (Tim m·∫°ch)                                                                          0                             27                                   0                      0                                    0                   0                            0               0                   0              0\n",
      "Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)                                                                     0                              1                                  24                      0                                    0                   0                            0               0                   0              0\n",
      "Th·ªßy ƒë·∫≠u (Chickenpox)                                                                                  0                              0                                   0                     24                                    0                   0                            0               0                   0              0\n",
      "B·ªánh Ph·ªïi t·∫Øc ngh·∫Ωn m·∫°n t√≠nh (COPD)                                                                    0                              0                                   0                      0                                   24                   0                            0               0                   0              0\n",
      "Vi√™m kh·ªõp m·∫°n t√≠nh                                                                                     0                              0                                   0                      0                                    0                  23                            0               0                   0              0\n",
      "ƒê√°i th√°o ƒë∆∞·ªùng (Ti·ªÉu ƒë∆∞·ªùng)                                                                            0                              0                                   0                      0                                    0                   0                           22               0                   0              0\n",
      "S·ªët xu·∫•t huy·∫øt                                                                                         0                              0                                   0                      0                                    0                   0                            0              20                   0              0\n",
      "B·ªánh Ti√™u ch·∫£y c·∫•p                                                                                     0                              0                                   0                      0                                    0                   0                            0               0                  20              0\n",
      "B·ªánh Lao ph·ªïi                                                                                          0                              0                                   0                      0                                    0                   0                            0               0                   0             19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üìä DETAILED EVALUATION - {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_pred_best = best_result['y_pred']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(\"\\n\" + classification_report(y_test, y_pred_best, zero_division=0))\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìà Per-class Performance:\")\n",
    "print(f\"\\n{'Disease':<45} {'ID':<8} {'Acc':>6} {'Samples':>8}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for disease in sorted(np.unique(y_test)):\n",
    "    mask = y_test == disease\n",
    "    if mask.sum() > 0:\n",
    "        acc = accuracy_score(y_test[mask], y_pred_best[mask])\n",
    "        disease_id = reverse_disease_map.get(disease, \"N/A\")\n",
    "        n_samples = mask.sum()\n",
    "        print(f\"{disease:<45} {disease_id:<8} {acc:>5.1%} {n_samples:>8}\")\n",
    "\n",
    "# Confusion Matrix (top 10 classes)\n",
    "print(\"\\nüìä Confusion Matrix (Top 10 most common classes in test set):\")\n",
    "unique_test_classes = np.unique(y_test)\n",
    "\n",
    "if len(unique_test_classes) > 10:\n",
    "    test_counts = Counter(y_test)\n",
    "    top_classes = [c for c, _ in test_counts.most_common(10)]\n",
    "    \n",
    "    mask = np.isin(y_test, top_classes)\n",
    "    y_test_top = y_test[mask]\n",
    "    y_pred_top = y_pred_best[mask]\n",
    "    \n",
    "    cm = confusion_matrix(y_test_top, y_pred_top, labels=top_classes)\n",
    "    cm_df = pd.DataFrame(cm, index=top_classes, columns=top_classes)\n",
    "else:\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "    cm_df = pd.DataFrame(cm, index=unique_test_classes, columns=unique_test_classes)\n",
    "\n",
    "print(\"\\n\" + cm_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üíæ SAVING MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Saved model: ../../models/naive_bayes_model.pkl\n",
      "‚úì Saved info: ../../models/model_info.json\n",
      "‚úì Saved comparison: ../../models/all_models_comparison.json\n",
      "\n",
      "üì¶ Saved files:\n",
      "   1. ../../models/naive_bayes_model.pkl\n",
      "   2. ../../models/model_info.json\n",
      "   3. ../../models/all_models_comparison.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "\n",
    "# 1. Save best model\n",
    "model_path = \"../../models/naive_bayes_model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\n‚úì Saved model: {model_path}\")\n",
    "\n",
    "# 2. Save model info\n",
    "model_info = {\n",
    "    \"model_name\": best_model_name,\n",
    "    \"model_type\": type(best_model).__name__,\n",
    "    \"symptoms\": all_symptoms,\n",
    "    \"diseases\": sorted(np.unique(y_train).tolist()),\n",
    "    \"disease_mapping\": disease_mapping,\n",
    "    \"metrics\": {\n",
    "        \"test_accuracy\": float(best_result['test_acc']),\n",
    "        \"precision\": float(best_result['precision']),\n",
    "        \"recall\": float(best_result['recall']),\n",
    "        \"f1_score\": float(best_result['f1']),\n",
    "        \"cv_accuracy_mean\": float(best_result['cv_mean']) if best_result['cv_mean'] > 0 else None,\n",
    "        \"cv_accuracy_std\": float(best_result['cv_std']) if best_result['cv_std'] > 0 else None\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"n_samples_train\": len(X_train),\n",
    "        \"n_samples_test\": len(X_test),\n",
    "        \"n_features\": len(all_symptoms),\n",
    "        \"n_classes\": len(np.unique(y_train)),\n",
    "        \"split_method\": split_method,\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"smoothing_value\": smoothing_value\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"training_time\": float(best_result['train_time']),\n",
    "        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"author\": \"phamlucchuong\"\n",
    "    }\n",
    "}\n",
    "\n",
    "info_path = \"../../models/model_info.json\"\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_info, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úì Saved info: {info_path}\")\n",
    "\n",
    "# 3. Save all model results for comparison\n",
    "all_results_path = \"../../models/all_models_comparison.json\"\n",
    "all_results_data = {\n",
    "    name: {\n",
    "        \"train_accuracy\": float(res['train_acc']),\n",
    "        \"test_accuracy\": float(res['test_acc']),\n",
    "        \"cv_mean\": float(res['cv_mean']) if res['cv_mean'] > 0 else None,\n",
    "        \"cv_std\": float(res['cv_std']) if res['cv_std'] > 0 else None,\n",
    "        \"precision\": float(res['precision']),\n",
    "        \"recall\": float(res['recall']),\n",
    "        \"f1_score\": float(res['f1']),\n",
    "        \"training_time\": float(res['train_time'])\n",
    "    }\n",
    "    for name, res in results.items()\n",
    "}\n",
    "\n",
    "with open(all_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results_data, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úì Saved comparison: {all_results_path}\")\n",
    "\n",
    "print(f\"\\nüì¶ Saved files:\")\n",
    "print(f\"   1. {model_path}\")\n",
    "print(f\"   2. {info_path}\")\n",
    "print(f\"   3. {all_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test v·ªõi d·ªØ li·ªáu m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING WITH NEW DATA\n",
      "======================================================================\n",
      "\n",
      "üìù Test Results:\n",
      "\n",
      "Case 1: C√∫m:\n",
      "   Symptoms: s·ªët_cao_tr√™n_38_5_ƒë·ªô, ƒëau_ƒë·∫ßu_d·ªØ_d·ªôi, ƒëau_nh·ª©c_c∆°_b·∫Øp ...\n",
      "   Expected: C·∫£m c√∫m (Influenza)\n",
      "   Predicted: C·∫£m c√∫m (D002) ‚ùå\n",
      "   \n",
      "   Top 5 predictions:\n",
      "   üëâ 1. C·∫£m c√∫m                                       (D002    ) 88.90%\n",
      "      2. S·ªët xu·∫•t huy·∫øt                                (D006    )  2.17%\n",
      "      3. Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p                  (D003    )  1.70%\n",
      "      4. S·ªët r√©t                                       (D007    )  0.99%\n",
      "      5. Vi√™m ph·∫ø qu·∫£n c·∫•p                             (D004    )  0.97%\n",
      "\n",
      "Case 2: C·∫£m l·∫°nh:\n",
      "   Symptoms: h·∫Øt_h∆°i, s·ªï_m≈©i, ngh·∫πt_m≈©i ...\n",
      "   Expected: C·∫£m l·∫°nh th√¥ng th∆∞·ªùng\n",
      "   Predicted: C·∫£m c√∫m (D002) ‚ùå\n",
      "   \n",
      "   Top 5 predictions:\n",
      "   üëâ 1. C·∫£m c√∫m                                       (D002    ) 44.23%\n",
      "      2. Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p                  (D003    ) 11.02%\n",
      "      3. Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            (D011    )  7.13%\n",
      "      4. C·∫£m l·∫°nh th√¥ng th∆∞·ªùng                         (D001    )  5.38%\n",
      "      5. B·ªánh Lao ph·ªïi                                 (D005    )  5.34%\n",
      "\n",
      "Case 3: Vi√™m h·ªçng:\n",
      "   Symptoms: ƒëau_h·ªçng_d·ªØ_d·ªôi, kh√≥_nu·ªët, s·ªët ...\n",
      "   Expected: Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p\n",
      "   Predicted: Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p (D003) ‚úÖ\n",
      "   \n",
      "   Top 5 predictions:\n",
      "   üëâ 1. Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p                  (D003    ) 74.07%\n",
      "      2. Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            (D011    )  3.59%\n",
      "      3. C·∫£m c√∫m                                       (D002    )  2.76%\n",
      "      4. C·∫£m l·∫°nh th√¥ng th∆∞·ªùng                         (D001    )  2.68%\n",
      "      5. B·ªánh Lao ph·ªïi                                 (D005    )  2.61%\n",
      "\n",
      "Case 4: Ti√™u ch·∫£y:\n",
      "   Symptoms: ti√™u_ch·∫£y_ph√¢n_l·ªèng, ƒëau_b·ª•ng_qu·∫∑n, s·ªët_nh·∫π ...\n",
      "   Expected: B·ªánh Ti√™u ch·∫£y c·∫•p\n",
      "   Predicted: Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis) (D011) ‚ùå\n",
      "   \n",
      "   Top 5 predictions:\n",
      "   üëâ 1. Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            (D011    ) 59.07%\n",
      "      2. Th·ªßy ƒë·∫≠u (Chickenpox)                         (D012    )  7.71%\n",
      "      3. Vi√™m kh·ªõp m·∫°n t√≠nh                            (D020    )  7.55%\n",
      "      4. B·ªánh Lao ph·ªïi                                 (D005    )  5.52%\n",
      "      5. S·ªët xu·∫•t huy·∫øt                                (D006    )  4.90%\n",
      "\n",
      "Case 5: G√∫t:\n",
      "   Symptoms: ƒëau_kh·ªõp_ng√≥n_ch√¢n_c√°i_d·ªØ_d·ªôi, s∆∞ng_ƒë·ªè_n√≥ng, ƒëau_ƒë·ªôt_ng·ªôt_ban_ƒë√™m\n",
      "   Expected: B·ªánh G√∫t (Gout)\n",
      "   Predicted: Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis) (D011) ‚ùå\n",
      "   \n",
      "   Top 5 predictions:\n",
      "   üëâ 1. Vi√™m d·∫° d√†y ru·ªôt (Gastroenteritis)            (D011    ) 13.58%\n",
      "      2. C·∫£m l·∫°nh th√¥ng th∆∞·ªùng                         (D001    )  9.94%\n",
      "      3. B·ªánh Lao ph·ªïi                                 (D005    )  9.27%\n",
      "      4. B·ªánh Tim m·∫°ch v√†nh (ƒêau th·∫Øt ng·ª±c, Nh·ªìi m√°u c∆° tim) (D014    )  7.92%\n",
      "      5. Vi√™m kh·ªõp m·∫°n t√≠nh                            (D020    )  7.42%\n",
      "\n",
      "======================================================================\n",
      "üìä Test Accuracy: 1/5 (20.0%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING WITH NEW DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def predict_disease(symptoms_list, model, all_symptoms, disease_mapping, top_k=5):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n b·ªánh t·ª´ danh s√°ch tri·ªáu ch·ª©ng\n",
    "    \n",
    "    Args:\n",
    "        symptoms_list: List of symptoms\n",
    "        model: Trained model\n",
    "        all_symptoms: List of all possible symptoms\n",
    "        disease_mapping: Dict mapping disease_id to disease_name\n",
    "        top_k: Number of top predictions\n",
    "    \n",
    "    Returns:\n",
    "        prediction, top_diseases, disease_id\n",
    "    \"\"\"\n",
    "    # Convert to binary vector\n",
    "    vector = np.array([1 if s in symptoms_list else 0 for s in all_symptoms])\n",
    "    vector = vector.reshape(1, -1) + smoothing_value\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(vector)[0]\n",
    "    probabilities = model.predict_proba(vector)[0]\n",
    "    \n",
    "    # Get disease_id\n",
    "    reverse_map = {v: k for k, v in disease_mapping.items()}\n",
    "    disease_id = reverse_map.get(prediction, \"Unknown\")\n",
    "    \n",
    "    # Top K\n",
    "    top_indices = np.argsort(probabilities)[::-1][:top_k]\n",
    "    top_diseases = []\n",
    "    for i in top_indices:\n",
    "        disease_name = model.classes_[i]\n",
    "        disease_id_top = reverse_map.get(disease_name, \"Unknown\")\n",
    "        top_diseases.append((disease_name, disease_id_top, probabilities[i]))\n",
    "    \n",
    "    return prediction, top_diseases, disease_id\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Case 1: C√∫m\",\n",
    "        \"symptoms\": [\"s·ªët_cao_tr√™n_38_5_ƒë·ªô\", \"ƒëau_ƒë·∫ßu_d·ªØ_d·ªôi\", \"ƒëau_nh·ª©c_c∆°_b·∫Øp\", \"ho_khan\"],\n",
    "        \"expected\": \"C·∫£m c√∫m (Influenza)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Case 2: C·∫£m l·∫°nh\",\n",
    "        \"symptoms\": [\"h·∫Øt_h∆°i\", \"s·ªï_m≈©i\", \"ngh·∫πt_m≈©i\", \"ƒëau_h·ªçng_nh·∫π\"],\n",
    "        \"expected\": \"C·∫£m l·∫°nh th√¥ng th∆∞·ªùng\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Case 3: Vi√™m h·ªçng\",\n",
    "        \"symptoms\": [\"ƒëau_h·ªçng_d·ªØ_d·ªôi\", \"kh√≥_nu·ªët\", \"s·ªët\", \"amidan_s∆∞ng_ƒë·ªè\"],\n",
    "        \"expected\": \"Vi√™m h·ªçng v√† Vi√™m Amidan c·∫•p\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Case 4: Ti√™u ch·∫£y\",\n",
    "        \"symptoms\": [\"ti√™u_ch·∫£y_ph√¢n_l·ªèng\", \"ƒëau_b·ª•ng_qu·∫∑n\", \"s·ªët_nh·∫π\", \"m·∫•t_n∆∞·ªõc\"],\n",
    "        \"expected\": \"B·ªánh Ti√™u ch·∫£y c·∫•p\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Case 5: G√∫t\",\n",
    "        \"symptoms\": [\"ƒëau_kh·ªõp_ng√≥n_ch√¢n_c√°i_d·ªØ_d·ªôi\", \"s∆∞ng_ƒë·ªè_n√≥ng\", \"ƒëau_ƒë·ªôt_ng·ªôt_ban_ƒë√™m\"],\n",
    "        \"expected\": \"B·ªánh G√∫t (Gout)\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Test Results:\\n\")\n",
    "\n",
    "correct = 0\n",
    "total = len(test_cases)\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    symptoms = test[\"symptoms\"]\n",
    "    expected = test.get(\"expected\", \"Unknown\")\n",
    "    \n",
    "    pred, top_diseases, disease_id = predict_disease(\n",
    "        symptoms, best_model, all_symptoms, disease_mapping\n",
    "    )\n",
    "    \n",
    "    is_correct = pred == expected\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f\"{test['name']}:\")\n",
    "    print(f\"   Symptoms: {', '.join(symptoms[:3])}\" + (\" ...\" if len(symptoms) > 3 else \"\"))\n",
    "    print(f\"   Expected: {expected}\")\n",
    "    print(f\"   Predicted: {pred} ({disease_id}) {'‚úÖ' if is_correct else '‚ùå'}\")\n",
    "    print(f\"   \\n   Top 5 predictions:\")\n",
    "    for rank, (disease, did, prob) in enumerate(top_diseases, 1):\n",
    "        marker = \"üëâ\" if disease == pred else \"  \"\n",
    "        print(f\"   {marker} {rank}. {disease:<45} ({did:<8}) {prob:>6.2%}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìä Test Accuracy: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset:\n",
      "   Total samples: 2173\n",
      "   Train samples: 1737\n",
      "   Test samples: 435\n",
      "   Features: 625\n",
      "   Classes: 20\n",
      "\n",
      "üèÜ Best Model:\n",
      "   Name: MultinomialNB (Œ±=0.1)\n",
      "   Test Accuracy: 98.62%\n",
      "   F1-Score: 98.63%\n",
      "\n",
      "üíæ Saved Files:\n",
      "   - naive_bayes_model.pkl\n",
      "   - model_info.json\n",
      "   - all_models_comparison.json\n",
      "\n",
      "‚è±Ô∏è  Training Info:\n",
      "   Training time: 0.0185s\n",
      "   Timestamp: 2025-11-19 11:50:49\n",
      "   Author: phamlucchuong\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DONE!\n",
      "======================================================================\n",
      "\n",
      "üìù Next steps:\n",
      "   1. Review model performance in model_info.json\n",
      "   2. Test model with API: python api/main.py\n",
      "   3. Deploy to production\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Train samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features: {len(all_symptoms)}\")\n",
    "print(f\"   Classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model:\")\n",
    "print(f\"   Name: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_result['test_acc']:.2%}\")\n",
    "print(f\"   F1-Score: {best_result['f1']:.2%}\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"   - naive_bayes_model.pkl\")\n",
    "print(f\"   - model_info.json\")\n",
    "print(f\"   - all_models_comparison.json\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Training Info:\")\n",
    "print(f\"   Training time: {best_result['train_time']:.4f}s\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Author: phamlucchuong\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DONE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìù Next steps:\")\n",
    "print(f\"   1. Review model performance in model_info.json\")\n",
    "print(f\"   2. Test model with API: python api/main.py\")\n",
    "print(f\"   3. Deploy to production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
