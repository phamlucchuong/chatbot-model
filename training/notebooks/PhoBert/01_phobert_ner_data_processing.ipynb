{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoBERT NER - X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "\n",
    "Notebook n√†y chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ format CSV sang format JSON ph√π h·ª£p v·ªõi PhoBERT.\n",
    "\n",
    "## Input\n",
    "- File: `../data/raw/train_data_ner.csv`\n",
    "- Format: CSV v·ªõi columns [sentence_id, token, label]\n",
    "\n",
    "## Output\n",
    "- `../data/processed/train_phobert.json` - Training set (80%)\n",
    "- `../data/processed/val_phobert.json` - Validation set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Set random seed ƒë·ªÉ reproducible\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ƒê·ªçc d·ªØ li·ªáu CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªçc 10642 tokens t·ª´ file CSV\n",
      "‚úì S·ªë l∆∞·ª£ng c√¢u: 1001\n",
      "\n",
      "D·ªØ li·ªáu m·∫´u:\n",
      "   sentence_id token      label\n",
      "0            1  Ch√†o          O\n",
      "1            1   b√°c          O\n",
      "2            1    sƒ©          O\n",
      "4            1   g·∫ßn          O\n",
      "5            1   ƒë√¢y          O\n",
      "6            1   t√¥i          O\n",
      "7            1    b·ªã          O\n",
      "8            1  ch·∫£y  B-SYMPTOM\n",
      "9            1  n∆∞·ªõc  I-SYMPTOM\n",
      "10           1   m≈©i  I-SYMPTOM\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc file CSV\n",
    "df = pd.read_csv(\"../../data/raw/raw_data_ner.csv\")\n",
    "\n",
    "# Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a t·ª´ t√™n c·ªôt\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c d√≤ng NaN\n",
    "df = df.dropna(subset=['sentence_id', 'token', 'label'])\n",
    "\n",
    "print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tokens t·ª´ file CSV\")\n",
    "print(f\"‚úì S·ªë l∆∞·ª£ng c√¢u: {df['sentence_id'].nunique()}\")\n",
    "print(\"\\nD·ªØ li·ªáu m·∫´u:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chuy·ªÉn ƒë·ªïi sang format PhoBERT\n",
    "\n",
    "Format PhoBERT c·∫ßn:\n",
    "```json\n",
    "{\n",
    "  \"tokens\": [\"T√¥i\", \"b·ªã\", \"ƒëau\", \"ƒë·∫ßu\"],\n",
    "  \"ner_tags\": [\"O\", \"O\", \"B-SYMPTOM\", \"I-SYMPTOM\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ chuy·ªÉn ƒë·ªïi 1001 c√¢u sang format PhoBERT\n",
      "\n",
      "V√≠ d·ª• d·ªØ li·ªáu sau khi chuy·ªÉn ƒë·ªïi:\n",
      "\n",
      "C√¢u 1:\n",
      "  Tokens: ['Ch√†o', 'b√°c', 'sƒ©', 'g·∫ßn', 'ƒë√¢y', 't√¥i', 'b·ªã', 'ch·∫£y', 'n∆∞·ªõc', 'm≈©i', 'li√™n', 't·ª•c', 'v√†', 'ho', 'kh√†n', 'k√©o', 'd√†i']\n",
      "  Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM']\n",
      "\n",
      "C√¢u 2:\n",
      "  Tokens: ['T√¥i', 'b·ªã', 'ƒëau', 'm√¨nh', 'm·∫©y', 'v√†', 'nh·ª©c', 'm·ªèi', 'to√†n', 'th√¢n']\n",
      "  Labels: ['O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM']\n",
      "\n",
      "C√¢u 3:\n",
      "  Tokens: ['T√¥i', 'b·ªã', 'ho', 'k√©o', 'd√†i', 'v√†', 'b·ªã', 'ngh·∫πt', 'm≈©i']\n",
      "  Labels: ['O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM']\n"
     ]
    }
   ],
   "source": [
    "def convert_to_phobert_format(dataframe):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi DataFrame sang format JSON cho PhoBERT\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame v·ªõi columns [sentence_id, token, label]\n",
    "    \n",
    "    Returns:\n",
    "        List of dict v·ªõi format {\"tokens\": [...], \"ner_tags\": [...]}\n",
    "    \"\"\"\n",
    "    phobert_data = []\n",
    "    \n",
    "    # Nh√≥m theo sentence_id\n",
    "    grouped = dataframe.groupby('sentence_id')\n",
    "    \n",
    "    for sentence_id, group in grouped:\n",
    "        tokens = group['token'].tolist()\n",
    "        labels = group['label'].tolist()\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o token l√† string\n",
    "        tokens = [str(token).strip() for token in tokens]\n",
    "        labels = [str(label).strip() for label in labels]\n",
    "        \n",
    "        phobert_data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": labels\n",
    "        })\n",
    "    \n",
    "    return phobert_data\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi\n",
    "all_data = convert_to_phobert_format(df)\n",
    "\n",
    "print(f\"‚úì ƒê√£ chuy·ªÉn ƒë·ªïi {len(all_data)} c√¢u sang format PhoBERT\")\n",
    "print(\"\\nV√≠ d·ª• d·ªØ li·ªáu sau khi chuy·ªÉn ƒë·ªïi:\")\n",
    "for i, item in enumerate(all_data[:3]):\n",
    "    print(f\"\\nC√¢u {i+1}:\")\n",
    "    print(f\"  Tokens: {item['tokens']}\")\n",
    "    print(f\"  Labels: {item['ner_tags']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Th·ªëng k√™ labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Th·ªëng k√™ labels:\n",
      "T·ªïng s·ªë labels duy nh·∫•t: 8\n",
      "\n",
      "Ph√¢n b·ªë labels:\n",
      "  O: 4460 tokens (41.9%)\n",
      "  I-SYMPTOM: 4180 tokens (39.3%)\n",
      "  B-SYMPTOM: 1920 tokens (18.0%)\n",
      "  I-DISEASE: 38 tokens (0.4%)\n",
      "  T√¥i: 21 tokens (0.2%)\n",
      "  B-DISEASE: 20 tokens (0.2%)\n",
      "  I-I-SYMPTOM: 2 tokens (0.0%)\n",
      "  I-SYMPTEM: 1 tokens (0.0%)\n",
      "\n",
      "‚úì Danh s√°ch labels: ['B-DISEASE', 'B-SYMPTOM', 'I-DISEASE', 'I-I-SYMPTOM', 'I-SYMPTEM', 'I-SYMPTOM', 'O', 'T√¥i']\n"
     ]
    }
   ],
   "source": [
    "# ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói lo·∫°i label\n",
    "label_counts = defaultdict(int)\n",
    "unique_labels = set()\n",
    "\n",
    "for item in all_data:\n",
    "    for label in item['ner_tags']:\n",
    "        label_counts[label] += 1\n",
    "        unique_labels.add(label)\n",
    "\n",
    "print(\"üìä Th·ªëng k√™ labels:\")\n",
    "print(f\"T·ªïng s·ªë labels duy nh·∫•t: {len(unique_labels)}\")\n",
    "print(\"\\nPh√¢n b·ªë labels:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label}: {count} tokens ({count/sum(label_counts.values())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Danh s√°ch labels: {sorted(unique_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Train/Validation (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training set: 800 c√¢u (79.9%)\n",
      "‚úì Validation set: 201 c√¢u (20.1%)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# T√≠nh split point\n",
    "split_idx = int(len(all_data) * 0.8)\n",
    "\n",
    "train_data = all_data[:split_idx]\n",
    "val_data = all_data[split_idx:]\n",
    "\n",
    "print(f\"‚úì Training set: {len(train_data)} c√¢u ({len(train_data)/len(all_data)*100:.1f}%)\")\n",
    "print(f\"‚úì Validation set: {len(val_data)} c√¢u ({len(val_data)/len(all_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. L∆∞u d·ªØ li·ªáu v√†o file JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ l∆∞u training set v√†o: ../../data/processed/train_phobert.json\n",
      "‚úì ƒê√£ l∆∞u validation set v√†o: ../../data/processed/val_phobert.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(\"../../data/processed\", exist_ok=True)\n",
    "\n",
    "# L∆∞u training set\n",
    "train_path = \"../../data/processed/train_phobert.json\"\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì ƒê√£ l∆∞u training set v√†o: {train_path}\")\n",
    "\n",
    "# L∆∞u validation set\n",
    "val_path = \"../../data/processed/val_phobert.json\"\n",
    "with open(val_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì ƒê√£ l∆∞u validation set v√†o: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ l∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ l∆∞u:\n",
      "  Training: 800 c√¢u\n",
      "  Validation: 201 c√¢u\n",
      "\n",
      "‚úÖ Ho√†n th√†nh x·ª≠ l√Ω d·ªØ li·ªáu!\n",
      "\n",
      "üìù B∆∞·ªõc ti·∫øp theo: Ch·∫°y notebook 02_train_phobert_ner.ipynb ƒë·ªÉ train model\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc l·∫°i ƒë·ªÉ verify\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    loaded_train = json.load(f)\n",
    "\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    loaded_val = json.load(f)\n",
    "\n",
    "print(\"‚úì Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ l∆∞u:\")\n",
    "print(f\"  Training: {len(loaded_train)} c√¢u\")\n",
    "print(f\"  Validation: {len(loaded_val)} c√¢u\")\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n th√†nh x·ª≠ l√Ω d·ªØ li·ªáu!\")\n",
    "print(\"\\nüìù B∆∞·ªõc ti·∫øp theo: Ch·∫°y notebook 02_train_phobert_ner.ipynb ƒë·ªÉ train model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
