{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoBERT NER - Training Model\n",
    "\n",
    "Notebook n√†y fine-tune PhoBERT cho task Named Entity Recognition (NER) tr√™n d·ªØ li·ªáu y t·∫ø ti·∫øng Vi·ªát.\n",
    "\n",
    "## Model\n",
    "- Pretrained: `vinai/phobert-base` (135M parameters)\n",
    "- Task: Token Classification (NER)\n",
    "- Labels: B-SYMPTOM, I-SYMPTOM, B-DISEASE, I-DISEASE, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch version: 2.9.1+cpu\n",
      "‚úì CUDA available: False\n",
      "‚ö†Ô∏è  Ch·∫°y tr√™n CPU (s·∫Ω ch·∫≠m h∆°n)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ch·∫°y tr√™n CPU (s·∫Ω ch·∫≠m h∆°n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training: 320 c√¢u\n",
      "‚úì Validation: 80 c√¢u\n",
      "\n",
      "V√≠ d·ª•:\n",
      "{'tokens': ['Vi√™m', 'tai', 'gi·ªØa'], 'ner_tags': ['B-DISEASE', 'I-DISEASE', 'I-DISEASE']}\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc training data\n",
    "with open(\"../../data/processed/train_phobert.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# ƒê·ªçc validation data\n",
    "with open(\"../../data/processed/val_phobert.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"‚úì Training: {len(train_data)} c√¢u\")\n",
    "print(f\"‚úì Validation: {len(val_data)} c√¢u\")\n",
    "\n",
    "print(\"\\nV√≠ d·ª•:\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T·∫°o Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì S·ªë l∆∞·ª£ng labels: 5\n",
      "‚úì Labels: ['O', 'B-DISEASE', 'B-SYMPTOM', 'I-DISEASE', 'I-SYMPTOM']\n",
      "\n",
      "Label mapping:\n",
      "  O ‚Üí 0\n",
      "  B-DISEASE ‚Üí 1\n",
      "  B-SYMPTOM ‚Üí 2\n",
      "  I-DISEASE ‚Üí 3\n",
      "  I-SYMPTOM ‚Üí 4\n"
     ]
    }
   ],
   "source": [
    "# T·ª± ƒë·ªông t√¨m t·∫•t c·∫£ labels t·ª´ data\n",
    "all_labels = set()\n",
    "for item in train_data + val_data:\n",
    "    all_labels.update(item['ner_tags'])\n",
    "\n",
    "# S·∫Øp x·∫øp labels (O lu√¥n ·ªü ƒë·∫ßu)\n",
    "label_list = sorted(list(all_labels))\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "\n",
    "# T·∫°o mapping\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"‚úì S·ªë l∆∞·ª£ng labels: {len(label_list)}\")\n",
    "print(f\"‚úì Labels: {label_list}\")\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} ‚Üí {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chuy·ªÉn ƒë·ªïi sang Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ t·∫°o Hugging Face Datasets\n",
      "  Train: 320 samples\n",
      "  Val: 80 samples\n"
     ]
    }
   ],
   "source": [
    "def convert_tags_to_ids(data, label2id):\n",
    "    \"\"\"Chuy·ªÉn text labels th√†nh IDs\"\"\"\n",
    "    converted = []\n",
    "    for item in data:\n",
    "        tag_ids = [label2id[tag] for tag in item['ner_tags']]\n",
    "        converted.append({\n",
    "            'tokens': item['tokens'],\n",
    "            'ner_tags': tag_ids\n",
    "        })\n",
    "    return converted\n",
    "\n",
    "train_data_converted = convert_tags_to_ids(train_data, label2id)\n",
    "val_data_converted = convert_tags_to_ids(val_data, label2id)\n",
    "\n",
    "# T·∫°o Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'tokens': [item['tokens'] for item in train_data_converted],\n",
    "    'ner_tags': [item['ner_tags'] for item in train_data_converted]\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'tokens': [item['tokens'] for item in val_data_converted],\n",
    "    'ner_tags': [item['ner_tags'] for item in val_data_converted]\n",
    "})\n",
    "\n",
    "print(\"‚úì ƒê√£ t·∫°o Hugging Face Datasets\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load PhoBERT Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ load PhoBERT model\n",
      "  Parameters: 134,411,525\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"vinai/phobert-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "print(\"‚úì ƒê√£ load tokenizer\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "print(\"‚úì ƒê√£ load PhoBERT model\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize v√† Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 1006.19 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:00<00:00, 1594.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ho√†n th√†nh tokenization\n",
      "\n",
      "üîç Ki·ªÉm tra tokenization:\n",
      "  Input IDs length: 5\n",
      "  Labels length: 5\n",
      "  Attention mask length: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text v√† align labels.\n",
    "    PhoBERT tokenizer kh√¥ng h·ªó tr·ª£ word_ids(), n√™n ta ph·∫£i x·ª≠ l√Ω th·ªß c√¥ng.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for tokens, labels in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "        # Tokenize t·ª´ng token ri√™ng l·∫ª\n",
    "        tokenized_tokens = []\n",
    "        aligned_labels = []\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            # Tokenize token hi·ªán t·∫°i\n",
    "            token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "            tokenized_tokens.extend(token_ids)\n",
    "            \n",
    "            # Label cho token ƒë·∫ßu ti√™n, -100 cho c√°c subword\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([-100] * (len(token_ids) - 1))\n",
    "        \n",
    "        # Th√™m special tokens [CLS] v√† [SEP]\n",
    "        input_ids = [tokenizer.cls_token_id] + tokenized_tokens + [tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels_with_special = [-100] + aligned_labels + [-100]\n",
    "        \n",
    "        # Truncate n·∫øu qu√° d√†i\n",
    "        max_length = 256\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            labels_with_special = labels_with_special[:max_length]\n",
    "        \n",
    "        tokenized_inputs[\"input_ids\"].append(input_ids)\n",
    "        tokenized_inputs[\"attention_mask\"].append(attention_mask)\n",
    "        tokenized_inputs[\"labels\"].append(labels_with_special)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"üîÑ Tokenizing data...\")\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=val_dataset.column_names)\n",
    "print(\"‚úì Ho√†n th√†nh tokenization\")\n",
    "\n",
    "# Ki·ªÉm tra 1 sample\n",
    "print(\"\\nüîç Ki·ªÉm tra tokenization:\")\n",
    "sample = tokenized_train[0]\n",
    "print(f\"  Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"  Labels length: {len(sample['labels'])}\")\n",
    "print(f\"  Attention mask length: {len(sample['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ t·∫°o data collator\n"
     ]
    }
   ],
   "source": [
    "# Data collator t·ª± ƒë·ªông padding batch\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "print(\"‚úì ƒê√£ t·∫°o data collator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a metrics function\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    T√≠nh precision, recall, F1 score cho NER task.\n",
    "    S·ª≠ d·ª•ng seqeval metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Chuy·ªÉn predictions v√† labels th√†nh text labels\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_pred = []\n",
    "        for pred, lab in zip(prediction, label):\n",
    "            if lab != -100:  # B·ªè qua padding v√† subword tokens\n",
    "                true_label.append(id2label[lab])\n",
    "                true_pred.append(id2label[pred])\n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_pred)\n",
    "\n",
    "    # T√≠nh metrics\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a metrics function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training configuration:\n",
      "  Learning rate: 2e-05\n",
      "  Batch size: 8\n",
      "  Epochs: 5\n",
      "  Weight decay: 0.01\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../models/phobert_ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../models/logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=2  # Ch·ªâ gi·ªØ 2 checkpoints t·ªët nh·∫•t\n",
    ")\n",
    "\n",
    "print(\"‚úì Training configuration:\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ kh·ªüi t·∫°o Trainer\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úì ƒê√£ kh·ªüi t·∫°o Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ B·∫Øt ƒë·∫ßu training...\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 41/200 00:42 < 02:54, 0.91 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.117700</td>\n",
       "      <td>0.973632</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ B·∫Øt ƒë·∫ßu training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Ho√†n th√†nh training!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:2790\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2795\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2796\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:3228\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3225\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3228\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3229\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:3325\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3323\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3324\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3325\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3328\u001b[39m     \u001b[38;5;66;03m# Wait for everyone to get here so we are sure the model has been saved by process 0\u001b[39;00m\n\u001b[32m   3329\u001b[39m     \u001b[38;5;66;03m# before we check if the best_checkpoint_dir exists\u001b[39;00m\n\u001b[32m   3330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:4227\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   4224\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   4226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m4227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   4230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\trainer.py:4336\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4331\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.save_pretrained(\n\u001b[32m   4332\u001b[39m         output_dir, state_dict=state_dict, safe_serialization=\u001b[38;5;28mself\u001b[39m.args.save_safetensors\n\u001b[32m   4333\u001b[39m     )\n\u001b[32m   4335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4337\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28mself\u001b[39m.data_collator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4339\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_collator, \u001b[33m\"\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4340\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_collator.tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4341\u001b[39m ):\n\u001b[32m   4342\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2452\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.save_pretrained\u001b[39m\u001b[34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[39m\n\u001b[32m   2449\u001b[39m         tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33mchat_template\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   2450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_config, saved_raw_chat_template_files\n\u001b[32m-> \u001b[39m\u001b[32m2452\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_pretrained\u001b[39m(\n\u001b[32m   2453\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2454\u001b[39m     save_directory: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m   2455\u001b[39m     legacy_format: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2456\u001b[39m     filename_prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2457\u001b[39m     push_to_hub: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2458\u001b[39m     **kwargs,\n\u001b[32m   2459\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, ...]:\n\u001b[32m   2460\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2461\u001b[39m \u001b[33;03m    Save the full tokenizer state.\u001b[39;00m\n\u001b[32m   2462\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2493\u001b[39m \u001b[33;03m        A tuple of `str`: The files saved.\u001b[39;00m\n\u001b[32m   2494\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2495\u001b[39m     use_auth_token = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_auth_token\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ B·∫Øt ƒë·∫ßu training...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Ho√†n th√†nh training!\")\n",
    "print(f\"\\nTraining metrics:\")\n",
    "print(f\"  Train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Train runtime: {train_result.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ƒê√°nh gi√° model tr√™n validation set...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà Validation Results:\n",
      "============================================================\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "Loss:      0.0194\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä ƒê√°nh gi√° model tr√™n validation set...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà Validation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall:    {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score:  {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Loss:      {eval_results['eval_loss']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. L∆∞u Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ l∆∞u model v√†o: ../../models/phobert_ner_model\n",
      "‚úì ƒê√£ l∆∞u label mapping\n"
     ]
    }
   ],
   "source": [
    "# L∆∞u model v√† tokenizer\n",
    "output_dir = \"../../models/phobert_ner_model\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úì ƒê√£ l∆∞u model v√†o: {output_dir}\")\n",
    "\n",
    "# L∆∞u label mapping\n",
    "import json\n",
    "with open(f\"{output_dir}/label_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì ƒê√£ l∆∞u label mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Model v·ªõi C√¢u M·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß™ TEST MODEL V·ªöI C√ÅC C√ÇU M·∫™U\n",
      "============================================================\n",
      "\n",
      "[1] Input: T√¥i b·ªã s·ªët cao v√† ho khan\n",
      "    Entities t√¨m ƒë∆∞·ª£c:\n",
      "      ‚Ä¢ s·ªët cao              [SYMPTOM] (score: 0.953)\n",
      "      ‚Ä¢ ho khan              [SYMPTOM] (score: 0.958)\n",
      "\n",
      "[2] Input: Em b√© c√≥ tri·ªáu ch·ª©ng ƒëau b·ª•ng v√† bu·ªìn n√¥n\n",
      "    Entities t√¨m ƒë∆∞·ª£c:\n",
      "      ‚Ä¢ ƒëau b·ª•ng             [SYMPTOM] (score: 0.972)\n",
      "      ‚Ä¢ bu·ªìn n√¥n             [SYMPTOM] (score: 0.972)\n",
      "\n",
      "[3] Input: C·∫£m c√∫m g√¢y ra c√°c tri·ªáu ch·ª©ng nh∆∞ s·ªët, ho v√† m·ªát m·ªèi\n",
      "    Entities t√¨m ƒë∆∞·ª£c:\n",
      "      ‚Ä¢ C·∫£m c√∫m              [DISEASE] (score: 0.909)\n",
      "      ‚Ä¢ s·ªë@@                 [SYMPTOM] (score: 0.877)\n",
      "      ‚Ä¢ ho                   [SYMPTOM] (score: 0.918)\n",
      "      ‚Ä¢ m·ªát m·ªèi              [SYMPTOM] (score: 0.965)\n",
      "\n",
      "[4] Input: T√¥i b·ªã ƒëau ƒë·∫ßu d·ªØ d·ªôi v√† ch√≥ng m·∫∑t\n",
      "    Entities t√¨m ƒë∆∞·ª£c:\n",
      "      ‚Ä¢ ƒëau ƒë·∫ßu              [SYMPTOM] (score: 0.950)\n",
      "      ‚Ä¢ d·ªØ d·ªôi               [SYMPTOM] (score: 0.843)\n",
      "      ‚Ä¢ ch√≥ng m·∫∑t            [SYMPTOM] (score: 0.967)\n",
      "\n",
      "[5] Input: B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng s·ªï m≈©i v√† ngh·∫πt m≈©i\n",
      "    Entities t√¨m ƒë∆∞·ª£c:\n",
      "      ‚Ä¢ s·ªï m≈©i               [DISEASE] (score: 0.685)\n",
      "      ‚Ä¢ ngh·∫πt m≈©i            [SYMPTOM] (score: 0.533)\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ HO√ÄN TH√ÄNH!\n",
      "\n",
      "üìù B∆∞·ªõc ti·∫øp theo: Ch·∫°y notebook 03_compare_spacy_phobert.ipynb ƒë·ªÉ so s√°nh v·ªõi Spacy\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# T·∫°o NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # G·ªôp B- v√† I- th√†nh 1 entity\n",
    ")\n",
    "\n",
    "# Test cases\n",
    "test_texts = [\n",
    "    \"T√¥i b·ªã s·ªët cao v√† ho khan\",\n",
    "    \"Em b√© c√≥ tri·ªáu ch·ª©ng ƒëau b·ª•ng v√† bu·ªìn n√¥n\",\n",
    "    \"C·∫£m c√∫m g√¢y ra c√°c tri·ªáu ch·ª©ng nh∆∞ s·ªët, ho v√† m·ªát m·ªèi\",\n",
    "    \"T√¥i b·ªã ƒëau ƒë·∫ßu d·ªØ d·ªôi v√† ch√≥ng m·∫∑t\",\n",
    "    \"B·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng s·ªï m≈©i v√† ngh·∫πt m≈©i\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß™ TEST MODEL V·ªöI C√ÅC C√ÇU M·∫™U\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n[{i}] Input: {text}\")\n",
    "    \n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    if entities:\n",
    "        print(\"    Entities t√¨m ƒë∆∞·ª£c:\")\n",
    "        for ent in entities:\n",
    "            print(f\"      ‚Ä¢ {ent['word']:<20} [{ent['entity_group']}] (score: {ent['score']:.3f})\")\n",
    "    else:\n",
    "        print(\"    (Kh√¥ng t√¨m th·∫•y entity n√†o)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "print(\"\\nüìù B∆∞·ªõc ti·∫øp theo: Ch·∫°y notebook 03_compare_spacy_phobert.ipynb ƒë·ªÉ so s√°nh v·ªõi Spacy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
