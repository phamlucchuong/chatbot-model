{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PhoBERT NER tr√™n GPU RTX 4050\n",
    "\n",
    "Training PhoBERT cho Named Entity Recognition v·ªõi GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç GPU Status\n",
      "============================================================\n",
      "PyTorch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "VRAM: 6.00 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç GPU Status\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
    "    device = \"cpu\"\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data...\n",
      "‚úì Train: 800 sentences\n",
      "‚úì Val: 201 sentences\n",
      "\n",
      "üìù Sample:\n",
      "Tokens: ['C√≥', 'ph·∫£i', 'S·ªët', 'ƒë√¥i', 'khi']...\n",
      "Tags: ['O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM']...\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "print(\"üìÇ Loading data...\")\n",
    "\n",
    "with open(\"../../data/processed/train_phobert.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/processed/val_phobert.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"‚úì Train: {len(train_data)} sentences\")\n",
    "print(f\"‚úì Val: {len(val_data)} sentences\")\n",
    "\n",
    "# Sample\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"Tokens: {train_data[0]['tokens'][:5]}...\")\n",
    "print(f\"Tags: {train_data[0]['ner_tags'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T·∫°o label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è  Labels (8): ['B-DISEASE', 'B-SYMPTOM', 'I-DISEASE', 'I-I-SYMPTOM', 'I-SYMPTEM', 'I-SYMPTOM', 'O', 'T√¥i']\n",
      "\n",
      "üìä Label mapping:\n",
      "  0: B-DISEASE\n",
      "  1: B-SYMPTOM\n",
      "  2: I-DISEASE\n",
      "  3: I-I-SYMPTOM\n",
      "  4: I-SYMPTEM\n",
      "  5: I-SYMPTOM\n",
      "  6: O\n",
      "  7: T√¥i\n"
     ]
    }
   ],
   "source": [
    "# Get all unique labels\n",
    "all_labels = set()\n",
    "for item in train_data + val_data:\n",
    "    all_labels.update(item['ner_tags'])\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"üè∑Ô∏è  Labels ({len(label_list)}): {label_list}\")\n",
    "print(f\"\\nüìä Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert labels sang IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Converted labels to IDs\n",
      "\n",
      "Sample after conversion:\n",
      "Tokens: ['C√≥', 'ph·∫£i', 'S·ªët', 'ƒë√¥i', 'khi']\n",
      "Tag IDs: [6, 6, 1, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to IDs\n",
    "def convert_labels_to_ids(data, label2id):\n",
    "    converted = []\n",
    "    for item in data:\n",
    "        converted.append({\n",
    "            \"tokens\": item[\"tokens\"],\n",
    "            \"ner_tags\": [label2id[label] for label in item[\"ner_tags\"]]\n",
    "        })\n",
    "    return converted\n",
    "\n",
    "train_data = convert_labels_to_ids(train_data, label2id)\n",
    "val_data = convert_labels_to_ids(val_data, label2id)\n",
    "\n",
    "print(\"‚úì Converted labels to IDs\")\n",
    "print(f\"\\nSample after conversion:\")\n",
    "print(f\"Tokens: {train_data[0]['tokens'][:5]}\")\n",
    "print(f\"Tag IDs: {train_data[0]['ner_tags'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. T·∫°o Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created HF Datasets\n",
      "\n",
      "üìä Dataset info:\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 800\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert to HF Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"‚úì Created HF Datasets\")\n",
    "print(f\"\\nüìä Dataset info:\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading vinai/phobert-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded on cuda\n",
      "Parameters: 134,413,832\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"vinai/phobert-base\"\n",
    "\n",
    "print(f\"üì• Loading {model_checkpoint}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded on {device}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize v√† align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:00<00:00, 5986.31 examples/s]\n",
      "\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 6564.88 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text v√† align labels v·ªõi subword tokens\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=256,\n",
    "        padding=False  # Kh√¥ng padding ·ªü ƒë√¢y, ƒë·ªÉ collator l√†m\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # PhoBERT tokenizer kh√¥ng c√≥ word_ids() ‚Üí ph·∫£i t·ª± align\n",
    "        word_ids = []\n",
    "        current_word_idx = 0\n",
    "        \n",
    "        # L·∫•y input_ids c·ªßa c√¢u n√†y\n",
    "        input_ids = tokenized_inputs[\"input_ids\"][i]\n",
    "        tokens_from_ids = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        for idx, token in enumerate(tokens_from_ids):\n",
    "            # Special tokens\n",
    "            if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
    "                word_ids.append(None)\n",
    "            # Subword token (b·∫Øt ƒë·∫ßu v·ªõi @@)\n",
    "            elif token.startswith(\"@@\"):\n",
    "                word_ids.append(current_word_idx - 1)  # Thu·ªôc t·ª´ tr∆∞·ªõc ƒë√≥\n",
    "            # Normal token\n",
    "            else:\n",
    "                word_ids.append(current_word_idx)\n",
    "                current_word_idx += 1\n",
    "        \n",
    "        # Align labels\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special token ‚Üí -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # T·ª´ m·ªõi ‚Üí g√°n label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            # Subword c·ªßa t·ª´ c≈© ‚Üí -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"üîÑ Tokenizing...\")\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(\"‚úì Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data collator ready\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Metrics function ready\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        for pred, lab in zip(prediction, label):\n",
    "            if lab != -100:\n",
    "                true_label.append(id2label[lab])\n",
    "                pred_label.append(id2label[pred])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "print(\"‚úì Metrics function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Arguments - T·ªëi ∆∞u cho RTX 4050 (6GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Arguments:\n",
      "   Device: cuda:0\n",
      "   FP16: True\n",
      "   Batch size: 16\n",
      "   Gradient accumulation: 2\n",
      "   Effective batch: 32\n",
      "   Epochs: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../models/phobert_ner_checkpoints\",\n",
    "    \n",
    "    # ===== GPU OPTIMIZATION =====\n",
    "    per_device_train_batch_size=16,      # RTX 4050 6GB c√≥ th·ªÉ handle\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,        # Effective batch = 16 * 2 = 32\n",
    "    fp16=True,                            # Mixed Precision (ti·∫øt ki·ªám VRAM)\n",
    "    dataloader_num_workers=4,             # TƒÉng t·ªëc data loading\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # ===== TRAINING PARAMS =====\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # ===== EVALUATION ===== (ƒê√É S·ª¨A)\n",
    "    eval_strategy=\"epoch\",                # ƒê·ªïi t·ª´ evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ===== LOGGING =====\n",
    "    logging_dir=\"../../models/logs\",\n",
    "    logging_steps=20,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # ===== SAVING =====\n",
    "    save_total_limit=2,  # Ch·ªâ gi·ªØ 2 checkpoint t·ªët nh·∫•t\n",
    "    \n",
    "    # ===== OTHER =====\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training Arguments:\")\n",
    "print(f\"   Device: {training_args.device}\")\n",
    "print(f\"   FP16: {training_args.fp16}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_30732\\2403826709.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer ready\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. START TRAINING üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ B·∫ÆT ƒê·∫¶U TRAINING\n",
      "============================================================\n",
      "üßπ GPU cache cleared\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 04:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.638200</td>\n",
       "      <td>0.702997</td>\n",
       "      <td>0.657471</td>\n",
       "      <td>0.718593</td>\n",
       "      <td>0.686675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.255503</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.844720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.211509</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.866834</td>\n",
       "      <td>0.859278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.856079</td>\n",
       "      <td>0.866834</td>\n",
       "      <td>0.861423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.193566</td>\n",
       "      <td>0.862843</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.866083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "============================================================\n",
      "Loss: 0.5220\n",
      "Time: 321.66s (5.4 mins)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU cache cleared\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Time: {train_result.metrics['train_runtime']:.2f}s ({train_result.metrics['train_runtime']/60:.1f} mins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results:\n",
      "   Precision: 0.8628\n",
      "   Recall: 0.8693\n",
      "   F1-score: 0.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: T√¥i seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Evaluating...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n‚úÖ Results:\")\n",
    "print(f\"   Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"   Recall: {eval_results['eval_recall']:.4f}\")\n",
    "print(f\"   F1-score: {eval_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving model...\n",
      "‚úÖ Saved to: ../../models/phobert_ner_model\n",
      "üßπ GPU cache cleared\n",
      "‚úÖ Saved to: ../../models/phobert_ner_model\n",
      "üßπ GPU cache cleared\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ Saving model...\")\n",
    "\n",
    "output_dir = \"../../models/phobert_ner_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save label mapping\n",
    "label_info = {\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": {str(k): v for k, v in id2label.items()}\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/label_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {output_dir}\")\n",
    "\n",
    "# Clear GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ TEST\n",
      "============================================================\n",
      "\n",
      "C√¢u test: T√¥i b·ªã s·ªët cao, ƒëau ƒë·∫ßu v√† ho nhi·ªÅu\n",
      "\n",
      "============================================================\n",
      "DEBUG: C√°ch PhoBERT tokenize c√¢u\n",
      "============================================================\n",
      "Input text: T√¥i b·ªã s·ªët cao, ƒëau ƒë·∫ßu v√† ho nhi·ªÅu\n",
      "Tokens t·ª´ tokenizer.tokenize(): ['T√¥i', 'b·ªã', 's·ªët', 'cao@@', ',', 'ƒëau', 'ƒë·∫ßu', 'v√†', 'ho', 'nhi·ªÅu']\n",
      "\n",
      "Tokens t·ª´ convert_ids_to_tokens(): ['<s>', 'T√¥i', 'b·ªã', 's·ªët', 'cao@@', ',', 'ƒëau', 'ƒë·∫ßu', 'v√†', 'ho', 'nhi·ªÅu', '</s>']\n",
      "\n",
      "============================================================\n",
      "K·∫æT QU·∫¢ (Tokens th√¥ t·ª´ PhoBERT):\n",
      "============================================================\n",
      "  T√¥i                  -> O\n",
      "  b·ªã                   -> O\n",
      "  s·ªët                  -> B-SYMPTOM\n",
      "  cao@@                -> I-SYMPTOM\n",
      "  ,                    -> O\n",
      "  ƒëau                  -> B-SYMPTOM\n",
      "  ƒë·∫ßu                  -> I-SYMPTOM\n",
      "  v√†                   -> O\n",
      "  ho                   -> B-SYMPTOM\n",
      "  nhi·ªÅu                -> I-SYMPTOM\n",
      "\n",
      "============================================================\n",
      "K·∫æT QU·∫¢ (ƒê√£ gh√©p subword - ƒê√öNG):\n",
      "============================================================\n",
      "  T√¥ib·ªãs·ªët             -> O\n",
      "  cao,ƒëauƒë·∫ßuv√†honhi·ªÅu  -> I-SYMPTOM\n",
      "\n",
      "‚úÖ Ho√†n t·∫•t!\n",
      "\n",
      "üìò Gi·∫£i th√≠ch PhoBERT tokenization:\n",
      "- PhoBERT c√≥ th·ªÉ ƒë√°nh d·∫•u @@ ·ªü 2 v·ªã tr√≠:\n",
      "  + Cu·ªëi token: 'cao@@' ‚Üí t·ª´ ch∆∞a ho√†n ch·ªânh, c√≥ subword ti·∫øp theo\n",
      "  + ƒê·∫ßu token: '@@t' ‚Üí subword ti·∫øp theo c·ªßa t·ª´ tr∆∞·ªõc\n",
      "- V√≠ d·ª•: 'information' ‚Üí ['in', '@@for', '@@mation']\n",
      "- Ho·∫∑c: 'cao nh·∫•t' ‚Üí ['cao@@', 'nh·∫•t'] (trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß™ TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_sentence = \"T√¥i b·ªã s·ªët cao, ƒëau ƒë·∫ßu v√† ho nhi·ªÅu\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Decode\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
    "\n",
    "print(f\"\\nC√¢u test: {test_sentence}\")\n",
    "\n",
    "# Debug: Xem c√°ch PhoBERT tokenize\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG: C√°ch PhoBERT tokenize c√¢u\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input text: {test_sentence}\")\n",
    "print(f\"Tokens t·ª´ tokenizer.tokenize(): {tokenizer.tokenize(test_sentence)}\")\n",
    "print(f\"\\nTokens t·ª´ convert_ids_to_tokens(): {tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K·∫æT QU·∫¢ (Tokens th√¥ t·ª´ PhoBERT):\")\n",
    "print(\"=\"*60)\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    if token not in ['<s>', '</s>', '<pad>']:\n",
    "        print(f\"  {token:20s} -> {label}\")\n",
    "\n",
    "# ===== DECODE ƒê√öNG: Gh√©p subword l·∫°i =====\n",
    "def decode_tokens_and_labels(tokens, labels):\n",
    "    \"\"\"Gh√©p subword tokens th√†nh t·ª´ g·ªëc (PhoBERT style)\"\"\"\n",
    "    words = []\n",
    "    word_labels = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        # B·ªè qua special tokens\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "        \n",
    "        # QUAN TR·ªåNG: PhoBERT ƒë√°nh d·∫•u @@ ·ªû CU·ªêI token (kh√¥ng ph·∫£i ƒë·∫ßu)\n",
    "        # V√≠ d·ª•: \"cao@@\" nghƒ©a l√† \"cao\" + subword ti·∫øp theo\n",
    "        if token.endswith('@@'):\n",
    "            # L∆∞u t·ª´ tr∆∞·ªõc (n·∫øu c√≥)\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                word_labels.append(current_label)\n",
    "            # Token n√†y k·∫øt th√∫c b·∫±ng @@ ‚Üí t·ª´ ch∆∞a ho√†n ch·ªânh\n",
    "            current_word = token[:-2]  # B·ªè @@\n",
    "            current_label = label\n",
    "        # Token b·∫Øt ƒë·∫ßu v·ªõi @@ (ti·∫øp t·ª•c t·ª´ tr∆∞·ªõc)\n",
    "        elif token.startswith('@@'):\n",
    "            current_word += token[2:]  # B·ªè @@\n",
    "        # Token th∆∞·ªùng\n",
    "        else:\n",
    "            # N·∫øu c√≥ t·ª´ tr∆∞·ªõc ch∆∞a ho√†n ch·ªânh ‚Üí gh√©p ti·∫øp\n",
    "            if current_word and current_word != \"\":\n",
    "                current_word += token\n",
    "            # N·∫øu kh√¥ng ‚Üí b·∫Øt ƒë·∫ßu t·ª´ m·ªõi\n",
    "            else:\n",
    "                # L∆∞u t·ª´ tr∆∞·ªõc (n·∫øu c√≥)\n",
    "                if current_word:\n",
    "                    words.append(current_word)\n",
    "                    word_labels.append(current_label)\n",
    "                current_word = token\n",
    "                current_label = label\n",
    "    \n",
    "    # L∆∞u t·ª´ cu·ªëi\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        word_labels.append(current_label)\n",
    "    \n",
    "    return words, word_labels\n",
    "\n",
    "words, word_labels = decode_tokens_and_labels(tokens, predicted_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K·∫æT QU·∫¢ (ƒê√£ gh√©p subword - ƒê√öNG):\")\n",
    "print(\"=\"*60)\n",
    "for word, label in zip(words, word_labels):\n",
    "    print(f\"  {word:20s} -> {label}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n t·∫•t!\")\n",
    "print(\"\\nüìò Gi·∫£i th√≠ch PhoBERT tokenization:\")\n",
    "print(\"- PhoBERT c√≥ th·ªÉ ƒë√°nh d·∫•u @@ ·ªü 2 v·ªã tr√≠:\")\n",
    "print(\"  + Cu·ªëi token: 'cao@@' ‚Üí t·ª´ ch∆∞a ho√†n ch·ªânh, c√≥ subword ti·∫øp theo\")\n",
    "print(\"  + ƒê·∫ßu token: '@@t' ‚Üí subword ti·∫øp theo c·ªßa t·ª´ tr∆∞·ªõc\")\n",
    "print(\"- V√≠ d·ª•: 'information' ‚Üí ['in', '@@for', '@@mation']\")\n",
    "print(\"- Ho·∫∑c: 'cao nh·∫•t' ‚Üí ['cao@@', 'nh·∫•t'] (trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
