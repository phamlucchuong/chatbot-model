{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e52f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CHATBOT MODEL\\training\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded PhoBERT tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "print(\"✓ Loaded PhoBERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494d3e8",
   "metadata": {},
   "source": [
    "## Test với từ \"sốt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2b7082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: sốt\n",
      "Tokens: ['sốt']\n",
      "Input IDs: [0, 2122, 2]\n",
      "Decoded tokens: ['<s>', 'sốt', '</s>']\n",
      "\n",
      "Vấn đề: Từ 'sốt' bị tách thành: ['sốt']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"sốt\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "input_ids = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Decoded tokens: {decoded_tokens}\")\n",
    "print(f\"\\nVấn đề: Từ 'sốt' bị tách thành: {decoded_tokens[1:-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5917f45",
   "metadata": {},
   "source": [
    "## Test với câu hoàn chỉnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2289adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu: Tôi bị sốt cao và đau đầu\n",
      "\n",
      "Tokens:\n",
      "  0: <s>\n",
      "  1: Tôi\n",
      "  2: bị\n",
      "  3: sốt\n",
      "  4: cao\n",
      "  5: và\n",
      "  6: đau\n",
      "  7: đầu\n",
      "  8: </s>\n",
      "\n",
      "Chú ý: Các token có '@@' là subword tokens (phần tiếp theo của từ trước đó)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Tôi bị sốt cao và đau đầu\"\n",
    "\n",
    "tokens = tokenizer.tokenize(test_sentence)\n",
    "input_ids = tokenizer.encode(test_sentence, add_special_tokens=True)\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "print(f\"Câu: {test_sentence}\")\n",
    "print(f\"\\nTokens:\")\n",
    "for i, token in enumerate(decoded_tokens):\n",
    "    print(f\"  {i}: {token}\")\n",
    "\n",
    "print(f\"\\nChú ý: Các token có '@@' là subword tokens (phần tiếp theo của từ trước đó)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04929ee3",
   "metadata": {},
   "source": [
    "## Giải thích vấn đề\n",
    "\n",
    "**PhoBERT sử dụng BPE (Byte-Pair Encoding):**\n",
    "- Từ được tách thành các subword\n",
    "- `@@` đánh dấu phần tiếp theo của từ (không phải đầu từ)\n",
    "- Ví dụ: `sốt` → `['s', '@@ố', '@@t']`\n",
    "\n",
    "**Vấn đề trong code:**\n",
    "- Khi in kết quả NER, bạn in trực tiếp tokens từ tokenizer\n",
    "- Cần ghép lại các subword thành từ gốc\n",
    "\n",
    "**Giải pháp:**\n",
    "1. Loại bỏ `@@` khi hiển thị\n",
    "2. Ghép các subword lại thành từ hoàn chỉnh\n",
    "3. Chỉ lấy label của token đầu tiên trong mỗi từ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d54fd",
   "metadata": {},
   "source": [
    "## Code sửa lỗi hiển thị"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2c20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả sau khi decode:\n",
      "  Tôi             -> O\n",
      "  bị              -> O\n",
      "  sốt             -> B-SYMPTOM\n",
      "  cao             -> O\n",
      "\n",
      "✅ Đã ghép 's' + '@@ố' + '@@t' = 'sốt'\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens_and_labels(tokens, labels):\n",
    "    \"\"\"\n",
    "    Ghép subword tokens thành từ gốc và lấy label tương ứng\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    word_labels = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        # Bỏ qua special tokens\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "        \n",
    "        # Subword token (tiếp tục từ trước)\n",
    "        if token.startswith('@@'):\n",
    "            current_word += token[2:]  # Bỏ @@\n",
    "        # Token mới (bắt đầu từ mới)\n",
    "        else:\n",
    "            # Lưu từ trước (nếu có)\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                word_labels.append(current_label)\n",
    "            \n",
    "            # Bắt đầu từ mới\n",
    "            current_word = token\n",
    "            current_label = label\n",
    "    \n",
    "    # Lưu từ cuối cùng\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        word_labels.append(current_label)\n",
    "    \n",
    "    return words, word_labels\n",
    "\n",
    "# Test\n",
    "test_tokens = ['<s>', 'Tôi', 'bị', 's', '@@ố', '@@t', 'cao', '</s>']\n",
    "test_labels = ['O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O']\n",
    "\n",
    "words, labels = decode_tokens_and_labels(test_tokens, test_labels)\n",
    "\n",
    "print(\"Kết quả sau khi decode:\")\n",
    "for word, label in zip(words, labels):\n",
    "    print(f\"  {word:15s} -> {label}\")\n",
    "\n",
    "print(\"\\n✅ Đã ghép 's' + '@@ố' + '@@t' = 'sốt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84c4a1",
   "metadata": {},
   "source": [
    "## Cập nhật code test trong notebook training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200eba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Cách hiển thị cũ (sai):\n",
      "  Tôi                  -> O\n",
      "  bị                   -> O\n",
      "  s                    -> B-SYMPTOM\n",
      "  @@ố                  -> I-SYMPTOM\n",
      "  @@t                  -> I-SYMPTOM\n",
      "  cao                  -> O\n",
      "\n",
      "✅ Cách hiển thị mới (đúng):\n",
      "  Tôi                  -> O\n",
      "  bị                   -> O\n",
      "  sốt                  -> B-SYMPTOM\n",
      "  cao                  -> O\n"
     ]
    }
   ],
   "source": [
    "# Code cũ (SAI):\n",
    "print(\"\\n❌ Cách hiển thị cũ (sai):\")\n",
    "test_tokens = ['<s>', 'Tôi', 'bị', 's', '@@ố', '@@t', 'cao', '</s>']\n",
    "test_labels = ['O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O']\n",
    "\n",
    "for token, label in zip(test_tokens, test_labels):\n",
    "    if token not in ['<s>', '</s>', '<pad>']:\n",
    "        print(f\"  {token:20s} -> {label}\")\n",
    "\n",
    "# Code mới (ĐÚNG):\n",
    "print(\"\\n✅ Cách hiển thị mới (đúng):\")\n",
    "words, word_labels = decode_tokens_and_labels(test_tokens, test_labels)\n",
    "for word, label in zip(words, word_labels):\n",
    "    print(f\"  {word:20s} -> {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
