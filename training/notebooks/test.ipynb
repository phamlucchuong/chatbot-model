{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83960351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b_train_ner_transformers.py\n",
    "# Hu·∫•n luy·ªán NER v·ªõi PhoBERT (transformers)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ªØ li·ªáu training\n",
    "data = {\n",
    "    \"tokens\": [\n",
    "        [\"T√¥i\", \"b·ªã\", \"s·ªët\", \"cao\", \"v√†\", \"ƒëau\", \"ƒë·∫ßu\"],\n",
    "        [\"Em\", \"b·ªã\", \"ho\", \"c√≥\", \"ƒë·ªùm\"],\n",
    "        [\"Con\", \"b·ªã\", \"ƒëau\", \"b·ª•ng\", \"v√†\", \"ti√™u\", \"ch·∫£y\"],\n",
    "        [\"B·ªánh\", \"nh√¢n\", \"c√≥\", \"s·ªët\", \"v√†\", \"bu·ªìn\", \"n√¥n\"],\n",
    "    ],\n",
    "    \"ner_tags\": [\n",
    "        [0, 0, 1, 2, 0, 1, 2],  # O, O, B-SYM, I-SYM, O, B-SYM, I-SYM\n",
    "        [0, 0, 1, 2, 2],\n",
    "        [0, 0, 1, 2, 0, 1, 2],\n",
    "        [0, 0, 0, 1, 0, 1, 2],\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Label mapping\n",
    "label_list = [\"O\", \"B-SYMPTOM\", \"I-SYMPTOM\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3563dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1090.92 examples/s]\n",
      "\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m     96\u001b[39m model = AutoModelForTokenClassification.from_pretrained(\n\u001b[32m     97\u001b[39m     model_checkpoint,\n\u001b[32m     98\u001b[39m     num_labels=\u001b[38;5;28mlen\u001b[39m(label_list),\n\u001b[32m     99\u001b[39m     id2label=id2label,\n\u001b[32m    100\u001b[39m     label2id=label2id\n\u001b[32m    101\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./ner_phobert_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Data collator\u001b[39;00m\n\u001b[32m    115\u001b[39m data_collator = DataCollatorForTokenClassification(tokenizer)\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load PhoBERT tokenizer\n",
    "model_checkpoint = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# if not getattr(tokenizer, \"is_fast\", False):\n",
    "#     raise ValueError(\"A fast tokenizer is required for label alignment (word_ids).\")\n",
    "\n",
    "# Tokenize v√† align labels (works with fast tokenizers and falls back to a\n",
    "# manual alignment for slow tokenizers that don't implement `word_ids()`)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # If the tokenizer is fast, use the convenient word_ids() method\n",
    "    if getattr(tokenizer, \"is_fast\", False):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            label_ids = []\n",
    "            previous_word_idx = None\n",
    "\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # Slow tokenizer fallback: build inputs and label alignment manually\n",
    "    # This works by tokenizing each word separately and assigning the label\n",
    "    # to the first token of the word, -100 to following subword tokens.\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "    labels_batch = []\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else (\n",
    "        tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    "    )\n",
    "\n",
    "    for words, word_labels in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "        ids = []\n",
    "        lbls = []\n",
    "\n",
    "        # add cls token if tokenizer has it\n",
    "        if tokenizer.cls_token_id is not None:\n",
    "            ids.append(tokenizer.cls_token_id)\n",
    "            lbls.append(-100)\n",
    "\n",
    "        for w_idx, word in enumerate(words):\n",
    "            # tokenize the single word (no special tokens)\n",
    "            word_pieces = tokenizer.tokenize(word)\n",
    "            if len(word_pieces) == 0:\n",
    "                # fallback: treat whole word as one token\n",
    "                token_ids = [tokenizer.unk_token_id if tokenizer.unk_token_id is not None else 0]\n",
    "            else:\n",
    "                token_ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "\n",
    "            ids.extend(token_ids)\n",
    "            # first piece gets the label, others get -100\n",
    "            lbls.append(word_labels[w_idx])\n",
    "            if len(token_ids) > 1:\n",
    "                lbls.extend([-100] * (len(token_ids) - 1))\n",
    "\n",
    "        # add sep token if tokenizer has it\n",
    "        if tokenizer.sep_token_id is not None:\n",
    "            ids.append(tokenizer.sep_token_id)\n",
    "            lbls.append(-100)\n",
    "\n",
    "        input_ids_batch.append(ids)\n",
    "        labels_batch.append(lbls)\n",
    "        attention_mask_batch.append([1] * len(ids))\n",
    "\n",
    "    # pad sequences to the same length\n",
    "    max_len = max(len(x) for x in input_ids_batch)\n",
    "    input_ids_padded = [ids + [pad_id] * (max_len - len(ids)) for ids in input_ids_batch]\n",
    "    attention_padded = [mask + [0] * (max_len - len(mask)) for mask in attention_mask_batch]\n",
    "    labels_padded = [lbl + [-100] * (max_len - len(lbl)) for lbl in labels_batch]\n",
    "\n",
    "    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_padded, \"labels\": labels_padded}\n",
    "\n",
    "# T·∫°o dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_phobert_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu training PhoBERT-NER...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./ner_phobert_model\")\n",
    "tokenizer.save_pretrained(\"./ner_phobert_model\")\n",
    "print(\"‚úì ƒê√£ l∆∞u model!\")\n",
    "\n",
    "# Test\n",
    "def predict_ner(text):\n",
    "    \"\"\"D·ª± ƒëo√°n NER cho c√¢u m·ªõi\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    results = []\n",
    "    for token, pred in zip(tokens, predictions[0]):\n",
    "        if token not in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
    "            results.append((token, id2label[pred.item()]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "test_text = \"T√¥i b·ªã s·ªët cao v√† ƒëau ƒë·∫ßu\"\n",
    "print(f\"\\nüìù Test: {test_text}\")\n",
    "print(predict_ner(test_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
